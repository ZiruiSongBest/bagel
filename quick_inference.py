#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ¨ç†è„šæœ¬ - ç”¨äºå¿«é€Ÿæµ‹è¯•æ‚¨è®­ç»ƒçš„è‡ªå›å½’äº¤é”™ç”Ÿæˆæ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
python quick_inference.py --prompt "ç”»ä¸€åªçŒ«åœ¨èŠ±å›­é‡Œç©è€"
"""

import os
import sys
import argparse
from pathlib import Path
import torch
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent))

from inference_unified_autoregressive import create_inference_engine


def quick_test(checkpoint_path, prompt, output_dir="quick_test_output"):
    """å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    print(f"ğŸš€ å¿«é€Ÿæ¨ç†æµ‹è¯•")
    print(f"æ£€æŸ¥ç‚¹: {checkpoint_path}")
    print(f"æç¤º: {prompt}")
    print("-" * 50)
    
    try:
        # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨: {checkpoint_path}")
            print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return False
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        engine = create_inference_engine(checkpoint_path)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æ‰§è¡Œæ¨ç†
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆ...")
        results = engine.autoregressive_generate(
            prompt=prompt,
            max_length=200,
            do_sample=True,
            temperature=0.8,
            image_shapes=(512, 512),  # ä½¿ç”¨è¾ƒå°å°ºå¯¸åŠ å¿«é€Ÿåº¦
            save_intermediate=True,
            output_dir=output_dir
        )
        
        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(results)} ä¸ªé¡¹ç›®")
        
        text_count = 0
        image_count = 0
        
        for i, item in enumerate(results):
            if isinstance(item, str):
                text_count += 1
                print(f"ğŸ“ [{i}] æ–‡æœ¬: {item[:100]}{'...' if len(item) > 100 else ''}")
            elif isinstance(item, Image.Image):
                image_count += 1
                print(f"ğŸ–¼ï¸  [{i}] å›¾åƒ: {item.size}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡: {text_count} ä¸ªæ–‡æœ¬, {image_count} ä¸ªå›¾åƒ")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ¨ç†æµ‹è¯•")
    
    parser.add_argument("--checkpoint_path", type=str,
                       default="/workspace/bagel/results/unified_training_20250910_201937/checkpoints/0000800",
                       help="æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--prompt", type=str,
                       default="ç”»ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€ï¼Œé˜³å…‰æ˜åªš",
                       help="è¾“å…¥æç¤º")
    parser.add_argument("--output_dir", type=str, default="quick_test_output",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"ğŸ”¥ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  ä½¿ç”¨CPUï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰")
    
    # æ‰§è¡Œå¿«é€Ÿæµ‹è¯•
    success = quick_test(args.checkpoint_path, args.prompt, args.output_dir)
    
    if success:
        print("\nğŸ‰ å¿«é€Ÿæµ‹è¯•æˆåŠŸï¼")
        print("æ‚¨å¯ä»¥æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç”Ÿæˆç»“æœ")
    else:
        print("\nğŸ’¥ å¿«é€Ÿæµ‹è¯•å¤±è´¥")
        print("è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œç¯å¢ƒé…ç½®")


if __name__ == "__main__":
    main()
