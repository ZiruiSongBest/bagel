#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•ç»Ÿä¸€å¤šæ¨¡æ€ç”ŸæˆåŠŸèƒ½

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªç®€åŒ–çš„æµ‹è¯•æ¥å£ï¼Œç”¨äºå¿«é€ŸéªŒè¯ç»Ÿä¸€ç”ŸæˆåŠŸèƒ½
"""

import torch
from PIL import Image
import json
from typing import List, Union, Dict, Any

def quick_test_unified_generation(inferencer, test_prompt: str = None):
    """
    å¿«é€Ÿæµ‹è¯•ç»Ÿä¸€ç”ŸæˆåŠŸèƒ½
    
    Args:
        inferencer: InterleaveInferencerå®ä¾‹
        test_prompt: æµ‹è¯•æç¤ºæ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤º
    """
    if test_prompt is None:
        test_prompt = "è¯·æè¿°ä¸€ä¸ªç¾ä¸½çš„æ—¥å‡ºåœºæ™¯ï¼Œç„¶åç”»å‡ºè¿™ä¸ªåœºæ™¯ã€‚"
    
    print("=== å¿«é€Ÿæµ‹è¯•ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆ ===")
    print(f"æµ‹è¯•æç¤º: {test_prompt}")
    print("-" * 50)
    
    try:
        # æµ‹è¯•1: è·å–åŸå§‹tokenåºåˆ—
        print("1. è·å–åŸå§‹tokenåºåˆ—...")
        raw_tokens = inferencer.unified_generate(
            input_text=test_prompt,
            max_length=100,
            do_sample=False,
            temperature=1.0,
            return_raw_tokens=True
        )
        
        print(f"ç”Ÿæˆäº† {len(raw_tokens)} ä¸ªtoken")
        print(f"å‰10ä¸ªtoken: {raw_tokens[:10]}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒtoken
        img_start_id = inferencer.new_token_ids.get('img_start_token_id')
        img_end_id = inferencer.new_token_ids.get('img_end_token_id')
        
        has_img_start = img_start_id in raw_tokens if img_start_id else False
        has_img_end = img_end_id in raw_tokens if img_end_id else False
        
        print(f"åŒ…å«å›¾åƒå¼€å§‹token: {has_img_start}")
        print(f"åŒ…å«å›¾åƒç»“æŸtoken: {has_img_end}")
        
    except Exception as e:
        print(f"åŸå§‹tokenç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    try:
        # æµ‹è¯•2: è·å–è§£æåçš„ç»“æœ
        print("\n2. è·å–è§£æåçš„ç»“æœ...")
        parsed_result = inferencer.unified_generate(
            input_text=test_prompt,
            max_length=100,
            do_sample=False,
            temperature=1.0,
            return_raw_tokens=False
        )
        
        print(f"è§£æåå…± {len(parsed_result)} ä¸ªå…ƒç´ ")
        for i, item in enumerate(parsed_result):
            if isinstance(item, str):
                print(f"æ–‡æœ¬å…ƒç´  {i}: {item[:50]}...")
            elif isinstance(item, Image.Image):
                print(f"å›¾åƒå…ƒç´  {i}: {item.size}")
            else:
                print(f"å…¶ä»–å…ƒç´  {i}: {type(item)}")
        
    except Exception as e:
        print(f"è§£æç»“æœæµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
    return True

def validate_token_ids(inferencer):
    """éªŒè¯å¿…è¦çš„token IDæ˜¯å¦å­˜åœ¨"""
    print("=== éªŒè¯Token IDé…ç½® ===")
    
    required_tokens = [
        'img_start_token_id',
        'img_end_token_id', 
        'eos_token_id',
        'bos_token_id'
    ]
    
    missing_tokens = []
    for token_name in required_tokens:
        if token_name not in inferencer.new_token_ids:
            missing_tokens.append(token_name)
        else:
            token_id = inferencer.new_token_ids[token_name]
            print(f"âœ… {token_name}: {token_id}")
    
    if missing_tokens:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„token ID: {missing_tokens}")
        return False
    else:
        print("âœ… æ‰€æœ‰å¿…è¦çš„token IDéƒ½å·²é…ç½®")
        return True

def test_token_prediction(inferencer, test_text: str = "Hello"):
    """æµ‹è¯•tokené¢„æµ‹åŠŸèƒ½"""
    print("=== æµ‹è¯•Tokené¢„æµ‹åŠŸèƒ½ ===")
    
    try:
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡
        gen_context = inferencer.init_gen_context()
        gen_context = inferencer.update_context_text(test_text, gen_context)
        
        # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
        dummy_input_ids = torch.tensor([[1]], device=inferencer.model.device)
        kv_lens = torch.tensor(gen_context['kv_lens'], dtype=torch.int, device=inferencer.model.device)
        ropes = torch.tensor(gen_context['ropes'], dtype=torch.long, device=inferencer.model.device)
        
        logits, updated_kv = inferencer._predict_next_token_logits(
            input_ids=dummy_input_ids,
            past_key_values=gen_context['past_key_values'],
            kv_lens=kv_lens,
            ropes=ropes
        )
        
        print(f"âœ… é¢„æµ‹logitså½¢çŠ¶: {logits.shape}")
        print(f"âœ… æ›´æ–°åçš„KV cacheç±»å‹: {type(updated_kv)}")
        return True
        
    except Exception as e:
        print(f"âŒ Tokené¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def comprehensive_test(inferencer):
    """ç»¼åˆæµ‹è¯•"""
    print("å¼€å§‹ç»¼åˆæµ‹è¯•ç»Ÿä¸€å¤šæ¨¡æ€ç”ŸæˆåŠŸèƒ½...")
    print("=" * 60)
    
    # æ­¥éª¤1: éªŒè¯é…ç½®
    if not validate_token_ids(inferencer):
        print("âŒ Token IDéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return False
    
    # æ­¥éª¤2: æµ‹è¯•tokené¢„æµ‹
    if not test_token_prediction(inferencer):
        print("âŒ Tokené¢„æµ‹æµ‹è¯•å¤±è´¥")
        return False
    
    # æ­¥éª¤3: å¿«é€Ÿç”Ÿæˆæµ‹è¯•
    if not quick_test_unified_generation(inferencer):
        print("âŒ ç»Ÿä¸€ç”Ÿæˆæµ‹è¯•å¤±è´¥")
        return False
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»Ÿä¸€å¤šæ¨¡æ€ç”ŸæˆåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    return True

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆ - å¿«é€Ÿæµ‹è¯•")
    print("è¯·ç¡®ä¿å·²æ­£ç¡®åŠ è½½æ¨¡å‹åè°ƒç”¨ comprehensive_test(inferencer)")
    
    # ç¤ºä¾‹è°ƒç”¨ä»£ç ï¼ˆéœ€è¦å®é™…çš„inferencerå®ä¾‹ï¼‰:
    """
    # å‡è®¾ä½ å·²ç»æœ‰ä¸€ä¸ªé…ç½®å¥½çš„inferencer
    # inferencer = your_loaded_inferencer
    # comprehensive_test(inferencer)
    """
