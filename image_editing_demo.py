#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾åƒç¼–è¾‘åŠŸèƒ½æ¼”ç¤ºDemo

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨BAGELæ¨¡å‹çš„unified generationåŠŸèƒ½è¿›è¡Œå›¾åƒç¼–è¾‘ã€‚
é€šè¿‡è¾“å…¥ä¸€å¼ å›¾åƒå’Œç¼–è¾‘æç¤ºè¯ï¼Œç”Ÿæˆç¼–è¾‘åçš„æ–°å›¾åƒã€‚
"""

import os
import torch
from PIL import Image
from typing import List, Union, Optional

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from data.data_utils import add_special_tokens
from data.transforms import ImageTransform
from modeling.autoencoder import load_ae
from modeling.bagel import (
    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,
    SiglipVisionConfig, SiglipVisionModel
)
from modeling.qwen2 import Qwen2Tokenizer
from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights


class ImageEditingDemo:
    """å›¾åƒç¼–è¾‘æ¼”ç¤ºç±»"""
    
    def __init__(self, inferencer):
        """
        Args:
            inferencer: InterleaveInferencerå®ä¾‹
        """
        self.inferencer = inferencer
    
    def edit_image_with_prompt(
        self, 
        input_image_path: str,
        edit_prompt: str,
        output_path: Optional[str] = None,
        image_size: tuple = (1024, 1024),
        cfg_text_scale: float = 4.0,
        cfg_img_scale: float = 1.5,
        timestep_shift: float = 3.0,
        num_timesteps: int = 50,
        enable_thinking: bool = True
    ) -> Image.Image:
        """
        ä½¿ç”¨æç¤ºè¯ç¼–è¾‘å›¾åƒ
        
        Args:
            input_image_path: è¾“å…¥å›¾åƒçš„è·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            output_path: è¾“å‡ºå›¾åƒä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            image_size: ç”Ÿæˆå›¾åƒçš„å°ºå¯¸
            cfg_text_scale: æ–‡æœ¬CFGç¼©æ”¾å› å­
            cfg_img_scale: å›¾åƒCFGç¼©æ”¾å› å­
            timestep_shift: æ—¶é—´æ­¥åç§»
            num_timesteps: å»å™ªæ­¥æ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            
        Returns:
            PIL.Image: ç¼–è¾‘åçš„å›¾åƒ
        """
        print(f"å¼€å§‹å›¾åƒç¼–è¾‘...")
        print(f"è¾“å…¥å›¾åƒ: {input_image_path}")
        print(f"ç¼–è¾‘æç¤º: {edit_prompt}")
        
        # åŠ è½½è¾“å…¥å›¾åƒ
        try:
            input_image = Image.open(input_image_path).convert('RGB')
            print(f"æˆåŠŸåŠ è½½å›¾åƒï¼Œå°ºå¯¸: {input_image.size}")
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ {input_image_path}: {e}")
        
        # æ„å»ºè¾“å…¥åºåˆ—ï¼šå›¾åƒ + æ–‡æœ¬æç¤º
        input_sequence = [input_image, edit_prompt]
        
        # ä½¿ç”¨interleave_inferenceè¿›è¡Œå›¾åƒç¼–è¾‘
        print("æ­£åœ¨ç”Ÿæˆç¼–è¾‘åçš„å›¾åƒ...")
        
        try:
            result = self.inferencer.interleave_inference(
                input_lists=input_sequence,
                think=enable_thinking,
                understanding_output=False,  # è®¾ä¸ºFalseä»¥ç”Ÿæˆå›¾åƒ
                max_think_token_n=500,
                do_sample=True,
                text_temperature=0.7,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                cfg_interval=[0.4, 1.0],
                timestep_shift=timestep_shift,
                num_timesteps=num_timesteps,
                cfg_renorm_min=0.0,
                cfg_renorm_type="global",
                image_shapes=image_size,
                enable_taylorseer=False,
            )
            
            print(f"ç”Ÿæˆå®Œæˆï¼ç»“æœåŒ…å« {len(result)} ä¸ªå…ƒç´ ")
            
            # æŸ¥æ‰¾ç”Ÿæˆçš„å›¾åƒ
            generated_image = None
            thinking_text = None
            
            for i, item in enumerate(result):
                if isinstance(item, str):
                    thinking_text = item
                    print(f"æ€è€ƒè¿‡ç¨‹: {item[:200]}...")
                elif isinstance(item, Image.Image):
                    generated_image = item
                    print(f"ç”Ÿæˆå›¾åƒ {i}: {item.size}")
            
            if generated_image is None:
                raise ValueError("æ²¡æœ‰ç”Ÿæˆå›¾åƒï¼")
            
            # ä¿å­˜ç»“æœ
            if output_path:
                generated_image.save(output_path)
                print(f"ç¼–è¾‘åçš„å›¾åƒå·²ä¿å­˜åˆ°: {output_path}")
            
            return generated_image, thinking_text
            
        except Exception as e:
            raise RuntimeError(f"å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    def test_force_generation(
        self,
        prompt: str,
        image_size: tuple = (1024, 1024),
        cfg_text_scale: float = 4.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
    ) -> List[Union[str, Image.Image]]:
        """
        æµ‹è¯•å¼ºåˆ¶å›¾åƒç”ŸæˆåŠŸèƒ½
        """
        print("ğŸ”¥ æµ‹è¯•å¼ºåˆ¶å›¾åƒç”Ÿæˆ")
        
        try:
            result = self.inferencer.test_force_image_generation(
                input_text=prompt,
                image_shapes=image_size,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
            )
            return result
        except Exception as e:
            print(f"å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
            return [prompt, f"[é”™è¯¯: {e}]"]
    
    def test_multi_step_generation(
        self,
        instruction: str,
        input_image_path: str = None,
        image_size: tuple = (1024, 1024),
        cfg_text_scale: float = 4.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        output_dir: str = "multi_step_outputs",
    ) -> List[Union[str, Image.Image]]:
        """
        æµ‹è¯•å¤šè½®å¼ºåˆ¶å›¾åƒç¼–è¾‘åŠŸèƒ½
        
        è¿™å°±æ˜¯ä½ è¦çš„åŠŸèƒ½ï¼åŸºäºè¾“å…¥å›¾åƒï¼Œæ ¹æ®å¤æ‚æŒ‡ä»¤å¼ºåˆ¶åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œæ¯æ­¥ç¼–è¾‘ç”Ÿæˆä¸€å¼ å›¾åƒ
        ä¾‹å¦‚ï¼šè¾“å…¥cat.jpg + æŒ‡ä»¤ "A cat wearing a hat fishing by the water, ink painting style" 
        å¼ºåˆ¶åˆ†è§£ç¼–è¾‘ï¼š
        - First, the cat wear a hat <image1> (åŸºäºåŸå›¾)
        - Second, the cat fishing by the water <image2> (åŸºäºimage1)
        - Third, transfer the style to ink painting style <image3> (åŸºäºimage2)
        """
        print("ğŸš€ æµ‹è¯•å¤šè½®å¼ºåˆ¶å›¾åƒç¼–è¾‘")
        print(f"å¤æ‚æŒ‡ä»¤: {instruction}")
        if input_image_path:
            print(f"è¾“å…¥å›¾åƒ: {input_image_path}")
        
        try:
            result = self.inferencer.force_multi_step_generation(
                instruction=instruction,
                input_image_path=input_image_path,  # ä¼ é€’è¾“å…¥å›¾åƒè·¯å¾„
                image_shapes=image_size,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
                save_intermediate=True,
                output_dir=output_dir,
            )
            
            # ç»Ÿè®¡ç”Ÿæˆçš„å›¾åƒæ•°é‡
            image_count = len([x for x in result if hasattr(x, 'size')])
            print(f"âœ… å¤šè½®ç”Ÿæˆå®Œæˆ! æ€»å…±ç”Ÿæˆäº† {image_count} å¼ å›¾åƒ")
            
            return result
            
        except Exception as e:
            print(f"å¤šè½®ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [instruction, f"[å¤šè½®ç”Ÿæˆé”™è¯¯: {e}]"]

    def unified_edit_with_text_generation(
        self,
        input_image_path: str, 
        edit_prompt: str,
        max_length: int = 800,
        temperature: float = 0.8,
        image_size: tuple = (1024, 1024),
        force_image_generation: bool = False,
    ) -> List[Union[str, Image.Image]]:
        """
        ä½¿ç”¨unified_generateè¿›è¡Œå›¾åƒç¼–è¾‘ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
        è¿™ä¸ªæ–¹æ³•å°è¯•åœ¨ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆè¿‡ç¨‹ä¸­åŒæ—¶å¤„ç†å›¾åƒè¾“å…¥å’Œæ–‡æœ¬è¾“å‡º
        
        Args:
            input_image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            image_size: å›¾åƒå°ºå¯¸
            
        Returns:
            åŒ…å«æ–‡æœ¬å’Œå›¾åƒçš„ç»“æœåˆ—è¡¨
        """
        print(f"ä½¿ç”¨unified_generateè¿›è¡Œå›¾åƒç¼–è¾‘...")
        
        # åŠ è½½å›¾åƒ
        input_image = Image.open(input_image_path).convert('RGB')
        
        # ç”±äºunified_generateç›®å‰ä¸»è¦æ”¯æŒæ–‡æœ¬è¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå°†å›¾åƒè½¬æ¢ä¸ºä¸Šä¸‹æ–‡
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå˜é€šçš„æ–¹æ³•
        
        # æ„å»ºåŒ…å«å›¾åƒä¿¡æ¯çš„æç¤ºè¯
        enhanced_prompt = f"åŸºäºæä¾›çš„å›¾åƒï¼Œ{edit_prompt}"
        
        try:
            # å…ˆç”¨å›¾åƒåˆå§‹åŒ–ä¸Šä¸‹æ–‡
            gen_context = self.inferencer.init_gen_context()
            from data.data_utils import pil_img2rgb
            processed_image = self.inferencer.vae_transform.resize_transform(
                pil_img2rgb(input_image)
            )
            gen_context = self.inferencer.update_context_image(
                processed_image, gen_context, vae=True, vit=True
            )
            
            # ç„¶åä½¿ç”¨æ–‡æœ¬ç”Ÿæˆ
            result = self.inferencer.unified_generate(
                input_text=enhanced_prompt,
                max_length=max_length,
                do_sample=True,
                temperature=temperature,
                image_shapes=image_size,
                cfg_text_scale=4.0,
                cfg_img_scale=1.5,
                return_raw_tokens=False,
                force_image_generation=force_image_generation,
                use_unified_system_prompt=True
            )
            
            return result
            
        except Exception as e:
            print(f"unifiedç¼–è¾‘æ–¹æ³•å‡ºé”™ï¼Œå›é€€åˆ°interleaveæ–¹æ³•: {e}")
            # å›é€€åˆ°æ ‡å‡†çš„interleaveæ–¹æ³•
            return self.edit_image_with_prompt(
                input_image_path, edit_prompt, image_size=image_size
            )


def load_bagel_model(model_path="models/BAGEL-7B-MoT", mode=1):
    """
    åŠ è½½BAGELæ¨¡å‹ï¼ˆå¤ç”¨unified_generation_example.pyä¸­çš„ä»£ç ï¼‰
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        mode: åŠ è½½æ¨¡å¼ (1=normal, 2=NF4, 3=INT8)
    
    Returns:
        tuple: (model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids)
    """
    print(f"å¼€å§‹åŠ è½½BAGELæ¨¡å‹ï¼Œè·¯å¾„: {model_path}")
    
    # åŠ è½½é…ç½®
    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, "llm_config.json"))
    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = "Qwen2MoTDecoderLayer"

    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, "vit_config.json"))
    vit_config.rope = False
    vit_config.num_hidden_layers -= 1

    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, "ae.safetensors"))

    config = BagelConfig(
        visual_gen=True,
        visual_und=True,
        llm_config=llm_config, 
        vit_config=vit_config,
        vae_config=vae_config,
        vit_max_num_patch_per_side=70,
        connector_act='gelu_pytorch_tanh',
        latent_patch_size=2,
        max_latent_size=64,
    )

    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    with init_empty_weights():
        language_model = Qwen2ForCausalLM(llm_config)
        vit_model = SiglipVisionModel(vit_config)
        model = Bagel(language_model, vit_model, config)
        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)

    # åŠ è½½tokenizer
    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)

    # åˆ›å»ºtransforms
    vae_transform = ImageTransform(1024, 512, 16)
    vit_transform = ImageTransform(980, 224, 14)

    # è®¾å¤‡æ˜ å°„å’Œæ¨¡å‹åŠ è½½
    device_map = infer_auto_device_map(
        model,
        max_memory={i: "80GiB" for i in range(torch.cuda.device_count())},
        no_split_module_classes=["Bagel", "Qwen2MoTDecoderLayer"],
    )

    same_device_modules = [
        'language_model.model.embed_tokens',
        'time_embedder',
        'latent_pos_embed',
        'vae2llm',
        'llm2vae',
        'connector',
        'vit_pos_embed'
    ]

    if torch.cuda.device_count() == 1:
        first_device = device_map.get(same_device_modules[0], "cuda:0")
        for k in same_device_modules:
            if k in device_map:
                device_map[k] = first_device
            else:
                device_map[k] = "cuda:0"
    else:
        first_device = device_map.get(same_device_modules[0])
        for k in same_device_modules:
            if k in device_map:
                device_map[k] = first_device

    # æ ¹æ®æ¨¡å¼åŠ è½½æ¨¡å‹æƒé‡
    if mode == 1:
        print("ä½¿ç”¨æ ‡å‡†æ¨¡å¼åŠ è½½æ¨¡å‹...")
        model = load_checkpoint_and_dispatch(
            model,
            checkpoint=os.path.join(model_path, "ema.safetensors"),
            device_map=device_map,
            offload_buffers=True,
            offload_folder="offload",
            dtype=torch.bfloat16,
            force_hooks=True,
        ).eval()
    elif mode == 2:  # NF4
        print("ä½¿ç”¨NF4é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹...")
        from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_4bit=True, 
            bnb_4bit_compute_dtype=torch.bfloat16, 
            bnb_4bit_use_double_quant=False, 
            bnb_4bit_quant_type="nf4"
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
    elif mode == 3:  # INT8
        print("ä½¿ç”¨INT8é‡åŒ–æ¨¡å¼åŠ è½½æ¨¡å‹...")
        from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_8bit=True, 
            torch_dtype=torch.float32
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
    else:
        raise NotImplementedError(f"æ¨¡å¼ {mode} æœªå®ç°")
    
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print(f"æ–°å¢çš„ç‰¹æ®Štoken ID: {new_token_ids}")
    
    return model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids


def main():
    """ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºå›¾åƒç¼–è¾‘åŠŸèƒ½"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å›¾åƒç¼–è¾‘åŠŸèƒ½æ¼”ç¤º")
    parser.add_argument("--model_path", type=str, default="models/BAGEL-7B-MoT",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--mode", type=int, default=1, choices=[1, 2, 3],
                       help="åŠ è½½æ¨¡å¼: 1=æ ‡å‡†, 2=NF4é‡åŒ–, 3=INT8é‡åŒ–")
    parser.add_argument("--input_image", type=str, 
                       default="/hdd_16T/Zirui/workspace/ATest/Bagel/assets/munchkin-cat-breed.jpg",
                       help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--prompt", type=str, 
                       default="A cat wearing a hat fishing by the water, ink painting style",
                       help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--output", type=str, default="edited_cat_fishing.png",
                       help="è¾“å‡ºå›¾åƒè·¯å¾„")
    parser.add_argument("--image_size", type=int, nargs=2, default=[1024, 1024],
                       help="ç”Ÿæˆå›¾åƒå°ºå¯¸ [å®½åº¦ é«˜åº¦]")
    parser.add_argument("--cfg_text_scale", type=float, default=4.0,
                       help="æ–‡æœ¬CFGç¼©æ”¾å› å­")
    parser.add_argument("--cfg_img_scale", type=float, default=1.5,
                       help="å›¾åƒCFGç¼©æ”¾å› å­")
    parser.add_argument("--timesteps", type=int, default=50,
                       help="å»å™ªæ­¥æ•°")
    parser.add_argument("--no_thinking", action="store_true",
                       help="ç¦ç”¨æ€è€ƒæ¨¡å¼")
    parser.add_argument("--test_force_generation", action="store_true",
                       help="æµ‹è¯•å¼ºåˆ¶å›¾åƒç”ŸæˆåŠŸèƒ½")
    parser.add_argument("--test_multi_step", action="store_true",
                       help="æµ‹è¯•å¤šè½®å¼ºåˆ¶å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ˆåŸºäºè¾“å…¥å›¾åƒå¼ºåˆ¶åˆ†è§£ä¸ºä¸‰ä¸ªç¼–è¾‘æ­¥éª¤ï¼‰")
    parser.add_argument("--test_unified", action="store_true",
                       help="æµ‹è¯•unified_generateæ–¹æ³•")
    parser.add_argument("--force_image_gen", action="store_true",
                       help="åœ¨unified_generateä¸­å¼ºåˆ¶å›¾åƒç”Ÿæˆ")
    parser.add_argument("--output_dir", type=str, default="multi_step_outputs",
                       help="å¤šè½®ç”Ÿæˆçš„è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("BAGEL å›¾åƒç¼–è¾‘åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    print(f"è¾“å…¥å›¾åƒ: {args.input_image}")
    print(f"ç¼–è¾‘æç¤º: {args.prompt}")
    print(f"è¾“å‡ºè·¯å¾„: {args.output}")
    print(f"å›¾åƒå°ºå¯¸: {args.image_size[0]}x{args.image_size[1]}")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥è¾“å…¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.input_image):
            raise FileNotFoundError(f"è¾“å…¥å›¾åƒä¸å­˜åœ¨: {args.input_image}")
        
        # åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids = load_bagel_model(
            model_path=args.model_path,
            mode=args.mode
        )
        
        # åˆ›å»ºinferencer
        from inferencer import InterleaveInferencer
        inferencer = InterleaveInferencer(
            model=model,
            vae_model=vae_model,
            tokenizer=tokenizer,
            vae_transform=vae_transform,
            vit_transform=vit_transform,
            new_token_ids=new_token_ids,
        )
        
        # åˆ›å»ºå›¾åƒç¼–è¾‘demoå®ä¾‹
        demo = ImageEditingDemo(inferencer)
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æµ‹è¯•æ¨¡å¼
        if args.test_force_generation:
            print("\nğŸ”¥ æµ‹è¯•å¼ºåˆ¶å›¾åƒç”ŸæˆåŠŸèƒ½...")
            result = demo.test_force_generation(
                prompt=args.prompt,
                image_size=tuple(args.image_size),
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.timesteps,
            )
            
            print(f"å¼ºåˆ¶ç”Ÿæˆç»“æœåŒ…å« {len(result)} ä¸ªå…ƒç´ ")
            for i, item in enumerate(result):
                if isinstance(item, Image.Image):
                    item.save(args.output)
                    print(f"ç”Ÿæˆçš„å›¾åƒå·²ä¿å­˜: {args.output}")
                elif isinstance(item, str):
                    print(f"æ–‡æœ¬å†…å®¹: {item}")
            return

        elif args.test_multi_step:
            print("\nğŸš€ æµ‹è¯•å¤šè½®å¼ºåˆ¶å›¾åƒç¼–è¾‘åŠŸèƒ½...")
            print("ğŸ¯ è¿™å°±æ˜¯ä½ è¦çš„åŠŸèƒ½ï¼åŸºäºè¾“å…¥å›¾åƒå¼ºåˆ¶åˆ†è§£ä¸ºä¸‰ä¸ªç¼–è¾‘æ­¥éª¤")
            print(f"ğŸ“ æŒ‡ä»¤: {args.prompt}")
            print(f"ğŸ–¼ï¸  è¾“å…¥å›¾åƒ: {args.input_image}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
            
            result = demo.test_multi_step_generation(
                instruction=args.prompt,
                input_image_path=args.input_image,  # ä¼ é€’è¾“å…¥å›¾åƒè·¯å¾„
                image_size=tuple(args.image_size),
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.timesteps,
                output_dir=args.output_dir,
            )
            
            print(f"\nğŸ“Š å¤šè½®ç”Ÿæˆç»“æœåŒ…å« {len(result)} ä¸ªå…ƒç´ ")
            image_count = 0
            for i, item in enumerate(result):
                if isinstance(item, Image.Image):
                    image_count += 1
                    print(f"ğŸ–¼ï¸  å›¾åƒ {image_count}: {item.size}")
                elif isinstance(item, str):
                    print(f"ğŸ“ æ­¥éª¤: {item}")
            
            print(f"\nğŸ‰ æ€»å…±ç¼–è¾‘ç”Ÿæˆäº† {image_count} å¼ å›¾åƒï¼Œæ‰€æœ‰å›¾åƒå·²ä¿å­˜åˆ° {args.output_dir}/ ç›®å½•")
            print("ğŸ“‹ å¼ºåˆ¶ç¼–è¾‘åˆ†è§£æ­¥éª¤:")
            print("  Step 1: Edit this image: First, the cat wear a hat <image1> (åŸºäºåŸå›¾)")
            print("  Step 2: Edit this image: Second, the cat fishing by the water <image2> (åŸºäºimage1)") 
            print("  Step 3: Edit this image: Third, transfer the style to ink painting style <image3> (åŸºäºimage2)")
            return

        elif args.test_unified:
            print("\nğŸ§ª æµ‹è¯•unified_generateæ–¹æ³•...")
            result = demo.unified_edit_with_text_generation(
                input_image_path=args.input_image,
                edit_prompt=args.prompt,
                image_size=tuple(args.image_size),
                force_image_generation=args.force_image_gen,
            )
            
            print(f"Unifiedç”Ÿæˆç»“æœåŒ…å« {len(result)} ä¸ªå…ƒç´ ")
            for i, item in enumerate(result):
                if isinstance(item, Image.Image):
                    item.save(args.output)
                    print(f"ç”Ÿæˆçš„å›¾åƒå·²ä¿å­˜: {args.output}")
                elif isinstance(item, str):
                    print(f"æ–‡æœ¬å†…å®¹: {item}")
            return
        
        else:
            # æ ‡å‡†å›¾åƒç¼–è¾‘æµç¨‹
            print("\nå¼€å§‹å›¾åƒç¼–è¾‘...")
            edited_image, thinking = demo.edit_image_with_prompt(
                input_image_path=args.input_image,
                edit_prompt=args.prompt,
                output_path=args.output,
                image_size=tuple(args.image_size),
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.timesteps,
                enable_thinking=not args.no_thinking
            )
        
        print("\n" + "=" * 60)
        print("å›¾åƒç¼–è¾‘å®Œæˆï¼")
        print(f"ç¼–è¾‘åçš„å›¾åƒå·²ä¿å­˜ä¸º: {args.output}")
        print(f"å›¾åƒå°ºå¯¸: {edited_image.size}")
        
        if thinking and not args.no_thinking:
            print("\næ¨¡å‹æ€è€ƒè¿‡ç¨‹:")
            print(thinking)
        
        print("=" * 60)
        
    except Exception as e:
        print(f"å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nè¯·ç¡®ä¿:")
        print("1. æ¨¡å‹è·¯å¾„æ­£ç¡®ä¸”æ¨¡å‹æ–‡ä»¶å®Œæ•´")
        print("2. è¾“å…¥å›¾åƒæ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»å–")
        print("3. æœ‰è¶³å¤Ÿçš„GPUæ˜¾å­˜")
        print("4. æ‰€æœ‰ä¾èµ–åº“å·²æ­£ç¡®å®‰è£…")


if __name__ == "__main__":
    main()
