#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“

åŸºäºBAGELæ¨¡å‹çš„å›¾åƒç¼–è¾‘æ¨ç†ï¼Œæ­£ç¡®çš„æ¨¡å‹åŠ è½½æ–¹å¼å‚è€ƒinferencer.py
ä¸“æ³¨äºå›¾åƒç¼–è¾‘æ¨¡å¼çš„æ¨ç†ï¼Œä¸åŒ…å«å¤æ‚çš„è‡ªå›å½’äº¤é”™ç”Ÿæˆé€»è¾‘ã€‚
"""

import os
import sys
import json
import torch
from pathlib import Path
from typing import List, Dict, Union, Optional, Tuple, Any
from PIL import Image
import numpy as np
import logging
import argparse
from copy import deepcopy
from contextlib import nullcontext

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(current_dir))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from data.data_utils import add_special_tokens, pil_img2rgb
from data.transforms import ImageTransform
from modeling.autoencoder import load_ae
from modeling.bagel import (
    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,
    SiglipVisionConfig, SiglipVisionModel
)
from modeling.qwen2 import Qwen2Tokenizer
from inferencer import InterleaveInferencer
from modeling.bagel.qwen2_navit import NaiveCache

# å¦‚æœéœ€è¦ä½¿ç”¨é‡åŒ–æˆ–å¤šGPU
from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model
from safetensors.torch import load_file

logger = logging.getLogger(__name__)


class UnifiedAutoregressiveInferencer:
    """
    ç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨ - çœŸæ­£å¯¹åº”è®­ç»ƒæ—¶çš„forward_autoregressive_trainingé€»è¾‘
    
    ä¸è®­ç»ƒçš„ä¸»è¦å¯¹åº”å…³ç³»ï¼š
    1. ç»Ÿä¸€åºåˆ—å»ºæ¨¡ï¼šæ–‡æœ¬token + ç‰¹æ®Štoken + å›¾åƒpatches
    2. é€tokenè‡ªå›å½’ç”Ÿæˆï¼ŒåŒ…æ‹¬ç‰¹æ®Štokené¢„æµ‹
    3. patchçº§åˆ«çš„å›¾åƒFlow Matchingç”Ÿæˆ
    4. ä¿æŒè®­ç»ƒæ¨ç†ä¸€è‡´çš„æ•°æ®æµ
    """
    
    def __init__(self, model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids):
        self.model = model
        self.vae_model = vae_model
        self.tokenizer = tokenizer
        self.vae_transform = vae_transform
        self.vit_transform = vit_transform
        self.new_token_ids = new_token_ids
        self.device = next(model.parameters()).device
        # åŸºäºå®˜æ–¹æ¨ç†å™¨æ„å»ºçš„è¾…åŠ©å®ä¾‹ï¼Œç”¨äºç›´æ¥å¤ç”¨å›¾åƒç”Ÿæˆæµæ°´çº¿
        self.interleave_helper = InterleaveInferencer(
            model=self.model,
            vae_model=self.vae_model,
            tokenizer=self.tokenizer,
            vae_transform=self.vae_transform,
            vit_transform=self.vit_transform,
            new_token_ids=self.new_token_ids,
        )
        
        # ç‰¹æ®Štoken IDs
        self.start_of_image = new_token_ids.get('start_of_image')
        self.end_of_image = new_token_ids.get('end_of_image')
        self.bos_token_id = new_token_ids.get('bos_token_id')
        self.eos_token_id = new_token_ids.get('eos_token_id')
        
        print(f"ğŸ”§ ç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - start_of_image: {self.start_of_image}")
        print(f"   - end_of_image: {self.end_of_image}")
    
    @torch.no_grad()
    def unified_autoregressive_inference(
        self,
        input_text: str,
        input_image: Optional[Image.Image] = None,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 1.0,
        image_shapes: tuple = (1024, 1024),
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        timestep_shift: float = 3.0,
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        ç»Ÿä¸€çš„è‡ªå›å½’æ¨ç†ï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        Args:
            input_text: è¾“å…¥æ–‡æœ¬ï¼ˆå¯¹åº”è®­ç»ƒæ—¶çš„input_textï¼‰
            input_image: è¾“å…¥å›¾åƒï¼ˆå¯¹åº”è®­ç»ƒæ—¶çš„input_imageï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: æ¸©åº¦å‚æ•°
            image_shapes: ç”Ÿæˆå›¾åƒå°ºå¯¸
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: Flow Matchingæ­¥æ•°
            timestep_shift: æ—¶é—´æ­¥åç§»
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆæ–‡æœ¬å’Œå›¾åƒäº¤é”™ï¼‰
        """
        print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€è‡ªå›å½’æ¨ç†")
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {input_text}")
        print(f"ğŸ–¼ï¸  è¾“å…¥å›¾åƒ: {input_image.size if input_image else 'None'}")
        
        # 1. å¤„ç†è¾“å…¥é˜¶æ®µï¼ˆå¯¹åº”è®­ç»ƒçš„_process_input_stageï¼‰
        encode_state = self._process_input_stage(input_text, input_image)

        # 2. ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼ˆå¯¹åº”è®­ç»ƒçš„_process_unified_autoregressive_trainingï¼‰
        generated_sequence = self._unified_autoregressive_generation(
            encode_state=encode_state,
            input_text=input_text,
            max_length=max_length,
            do_sample=do_sample,
            temperature=temperature,
            image_shapes=image_shapes,
            cfg_text_scale=cfg_text_scale,
            cfg_img_scale=cfg_img_scale,
            num_timesteps=num_timesteps,
            timestep_shift=timestep_shift,
            **kwargs
        )
        
        # 3. è§£æè¾“å‡ºåºåˆ—
        output_results = self._parse_generated_sequence(generated_sequence)
        
        print(f"âœ… ç»Ÿä¸€è‡ªå›å½’æ¨ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(output_results)} ä¸ªç»“æœ")
        return output_results
    
    def _process_input_stage(
        self,
        input_text: str,
        input_image: Optional[Image.Image],
    ) -> Dict[str, Any]:
        """æ„å»ºä¸å®˜æ–¹æ¨ç†å™¨ä¸€è‡´çš„ç¼–ç ä¸Šä¸‹æ–‡ï¼Œè¿”å›ç¼“å­˜çŠ¶æ€ä¸å…ƒä¿¡æ¯ã€‚"""

        gen_context = self.interleave_helper.init_gen_context()
        cfg_text_context = deepcopy(gen_context)
        cfg_img_context = deepcopy(gen_context)

        text_token_history: List[int] = []
        processed_image_tensor = None
        image_shape: Optional[Tuple[int, int]] = None

        if input_image is not None and input_image.mode != "RGB":
            input_image = input_image.convert("RGB")

        context_image = input_image

        autocast_ctx = (
            torch.autocast(device_type=self.device.type, dtype=torch.bfloat16)
            if self.device.type == "cuda" else nullcontext()
        )

        with autocast_ctx:
            if input_text:
                text_token_history = self.tokenizer.encode(input_text)
                cfg_text_context = deepcopy(gen_context)
                gen_context = self.interleave_helper.update_context_text(input_text, gen_context)
                cfg_img_context = self.interleave_helper.update_context_text(input_text, cfg_img_context)

            if context_image is not None:
                processed_tensor = self.vae_transform.resize_transform(pil_img2rgb(context_image))
                processed_image_tensor = processed_tensor
                gen_context = self.interleave_helper.update_context_image(processed_tensor, gen_context)
                image_shape = processed_tensor.size[::-1]
                cfg_text_context = deepcopy(gen_context)

        total_length = gen_context['kv_lens'][0] if gen_context['kv_lens'] else 0

        print(f"âœ… è¾“å…¥å¤„ç†å®Œæˆï¼Œåºåˆ—é•¿åº¦: {total_length}")
        print(f"ğŸ“„ æ–‡æœ¬tokens: {len(text_token_history)}")
        if image_shape is not None:
            num_patches = image_shape[0] // self.model.latent_downsample * image_shape[1] // self.model.latent_downsample
            print(f"ğŸ–¼ï¸  å›¾åƒpatches: {num_patches}")

        return {
            'gen_context': gen_context,
            'cfg_text_context': cfg_text_context,
            'cfg_img_context': cfg_img_context,
            'text_token_history': text_token_history,
            'context_image': context_image,
            'processed_image': processed_image_tensor,
            'sequence_length': total_length,
            'image_shape': image_shape,
        }

    def _generate_text_with_helper(
        self,
        input_text: str,
        context_image: Optional[Image.Image],
        image_shape: Tuple[int, int],
        max_length: int,
        do_sample: bool,
        temperature: float,
        initial_contexts: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """å¤ç”¨å®˜æ–¹æ¨ç†å™¨è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œå¹¶è¿”å›CFGæ‰€éœ€çš„ä¸Šä¸‹æ–‡ã€‚"""

        autocast_ctx = (
            torch.autocast(device_type=self.device.type, dtype=torch.bfloat16)
            if self.device.type == "cuda" else nullcontext()
        )

        if initial_contexts is not None:
            gen_context = deepcopy(initial_contexts['gen_context'])
            cfg_text_context = deepcopy(initial_contexts['cfg_text_context'])
            cfg_img_context = deepcopy(initial_contexts['cfg_img_context'])
            inferred_image_shape = initial_contexts.get('image_shape') or image_shape

            with autocast_ctx:
                text_output, token_ids = self.interleave_helper.gen_text(
                    gen_context,
                    max_length=max_length,
                    do_sample=do_sample,
                    temperature=temperature,
                    return_tokens=True,
                )
                if token_ids:
                    decoded_with_special = self.tokenizer.decode(token_ids, skip_special_tokens=False)
                else:
                    decoded_with_special = text_output

                gen_context_after_text = self.interleave_helper.update_context_text(decoded_with_special, gen_context)
        else:
            gen_context = self.interleave_helper.init_gen_context()
            cfg_text_context = deepcopy(gen_context)
            cfg_img_context = deepcopy(gen_context)

            with autocast_ctx:
                if input_text:
                    cfg_text_context = deepcopy(gen_context)
                    gen_context = self.interleave_helper.update_context_text(input_text, gen_context)
                    cfg_img_context = self.interleave_helper.update_context_text(input_text, cfg_img_context)

                inferred_image_shape = image_shape
                if context_image is not None:
                    processed_image = self.vae_transform.resize_transform(pil_img2rgb(context_image))
                    gen_context = self.interleave_helper.update_context_image(processed_image, gen_context)
                    inferred_image_shape = processed_image.size[::-1]
                    cfg_text_context = deepcopy(gen_context)

                text_output, token_ids = self.interleave_helper.gen_text(
                    gen_context,
                    max_length=max_length,
                    do_sample=do_sample,
                    temperature=temperature,
                    return_tokens=True,
                )

                # ä½¿ç”¨token idsé‡å»ºåŒ…å«ç‰¹æ®Štokençš„æ–‡æœ¬ï¼Œé˜²æ­¢ä¾‹å¦‚<|vision_start|>è¢«æˆªæ–­
                if token_ids:
                    decoded_with_special = self.tokenizer.decode(token_ids, skip_special_tokens=False)
                else:
                    decoded_with_special = text_output

                gen_context_after_text = self.interleave_helper.update_context_text(decoded_with_special, gen_context)

        token_ids = (token_ids or []) if 'token_ids' in locals() else []

        print("ğŸ§ª helperæ–‡æœ¬è¾“å‡º:", text_output)
        print("ğŸ§ª helper tokenåºåˆ—:", token_ids)

        return {
            'text': text_output,
            'text_with_special': decoded_with_special if 'decoded_with_special' in locals() else text_output,
            'tokens': token_ids,
            'gen_context': gen_context_after_text,
            'cfg_text_context': cfg_text_context,
            'cfg_img_context': cfg_img_context,
            'image_shape': inferred_image_shape,
        }

    def _unified_autoregressive_generation(
        self,
        encode_state: Dict[str, Any],
        input_text: str,
        max_length: int,
        do_sample: bool,
        temperature: float,
        image_shapes: tuple,
        cfg_text_scale: float,
        cfg_img_scale: float,
        num_timesteps: int,
        timestep_shift: float,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€tokenå¤„ç†é€»è¾‘
        
        è®­ç»ƒæ—¶çš„æ ¸å¿ƒé€»è¾‘ï¼š
        1. é€tokené¢„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬tokenå’Œç‰¹æ®Štokenï¼ˆ<vision_start>, <vision_end>ï¼‰
        2. æ¨¡å‹è‡ªå·±å†³å®šä½•æ—¶è¾“å‡º<vision_start>å¼€å§‹å›¾åƒç”Ÿæˆ
        3. åœ¨<vision_start>å’Œ<vision_end>ä¹‹é—´ï¼Œé€patchç”Ÿæˆå›¾åƒ
        4. æ¨¡å‹è‡ªå·±å†³å®šä½•æ—¶è¾“å‡º<vision_end>ç»“æŸå›¾åƒç”Ÿæˆ
        
        Returns:
            ç”Ÿæˆåºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«tokenç±»å‹å’Œå†…å®¹
        """
        print(f"ğŸ¯ å¼€å§‹ç»Ÿä¸€è‡ªå›å½’ç”Ÿæˆï¼Œæœ€å¤§é•¿åº¦: {max_length}")
        
        # å½“å‰åºåˆ—çŠ¶æ€
        generated_sequence = []
        text_history = list(encode_state.get('text_token_history', []))
        context_image = encode_state.get('context_image')

        cfg_interval = kwargs.get('cfg_interval', (0.4, 1.0))
        cfg_renorm_min = kwargs.get('cfg_renorm_min', 0.0)
        cfg_renorm_type = kwargs.get('cfg_renorm_type', "global")
        enable_taylorseer = kwargs.get('enable_taylorseer', False)

        initial_contexts = None
        if encode_state.get('gen_context') is not None:
            initial_contexts = {
                'gen_context': encode_state.get('gen_context'),
                'cfg_text_context': encode_state.get('cfg_text_context'),
                'cfg_img_context': encode_state.get('cfg_img_context'),
                'image_shape': encode_state.get('image_shape'),
            }

        helper_outputs = self._generate_text_with_helper(
            input_text=input_text,
            context_image=context_image,
            image_shape=image_shapes,
            max_length=max_length,
            do_sample=do_sample,
            temperature=temperature,
            initial_contexts=initial_contexts,
        )

        generated_tokens: List[int] = helper_outputs.get('tokens') or []
        gen_context_for_image = helper_outputs.get('gen_context')
        cfg_text_context = helper_outputs.get('cfg_text_context')
        cfg_img_context = helper_outputs.get('cfg_img_context')
        current_image_shape = helper_outputs.get('image_shape') or image_shapes
        current_image_shape = tuple(current_image_shape)

        if gen_context_for_image is None:
            gen_context_for_image = self.interleave_helper.init_gen_context()
        if cfg_text_context is None:
            cfg_text_context = deepcopy(gen_context_for_image)
        if cfg_img_context is None:
            cfg_img_context = deepcopy(gen_context_for_image)

        special_token_ids = {
            v for v in self.new_token_ids.values() if isinstance(v, int)
        }

        for step, token_id in enumerate(generated_tokens):
            if token_id == self.bos_token_id:
                continue

            if token_id == self.start_of_image:
                print(f"ç¬¬ {step+1} æ­¥: é¢„æµ‹ special, token_id: {token_id} (start_of_image)")
                print("ğŸ–¼ï¸  æ¨¡å‹å†³å®šå¼€å§‹å›¾åƒç”Ÿæˆ")
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'start_of_image'
                })
                text_history.append(token_id)

                generated_image = self._generate_image_with_helper(
                    image_shape=current_image_shape,
                    gen_context=gen_context_for_image,
                    cfg_text_context=cfg_text_context,
                    cfg_img_context=cfg_img_context,
                    cfg_text_scale=cfg_text_scale,
                    cfg_img_scale=cfg_img_scale,
                    num_timesteps=num_timesteps,
                    timestep_shift=timestep_shift,
                    cfg_interval=cfg_interval,
                    cfg_renorm_min=cfg_renorm_min,
                    cfg_renorm_type=cfg_renorm_type,
                    enable_taylorseer=enable_taylorseer,
                )
                generated_sequence.append({
                    'type': 'image',
                    'content': generated_image
                })
                generated_sequence.append({
                    'type': 'special_token',
                    'content': self.end_of_image,
                    'token_name': 'end_of_image'
                })
                text_history.append(self.end_of_image)
                print("ğŸ–¼ï¸  é€šè¿‡InterleaveInferencer.gen_imageå®Œæˆæ•´å›¾ç”Ÿæˆ")
                break

            if token_id == self.eos_token_id:
                print(f"ç¬¬ {step+1} æ­¥: é¢„æµ‹ special, token_id: {token_id}")
                print("ğŸ é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ")
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'eos'
                })
                text_history.append(token_id)
                break

            if token_id == self.end_of_image:
                print(f"ç¬¬ {step+1} æ­¥: é¢„æµ‹ special, token_id: {token_id}")
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id,
                    'token_name': 'end_of_image'
                })
                text_history.append(token_id)
                continue

            if token_id in special_token_ids:
                debug_name = {
                    self.bos_token_id: 'bos',
                    self.eos_token_id: 'eos',
                    self.start_of_image: 'start_of_image',
                    self.end_of_image: 'end_of_image',
                }.get(token_id, 'special')
                print(f"ç¬¬ {step+1} æ­¥: é¢„æµ‹ special, token_id: {token_id} ({debug_name})")
                generated_sequence.append({
                    'type': 'special_token',
                    'content': token_id
                })
                text_history.append(token_id)
                continue

            generated_sequence.append({
                'type': 'text_token',
                'content': token_id
            })
            text_history.append(token_id)

        print(f"âœ… è‡ªå›å½’ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(generated_sequence)} ä¸ªå…ƒç´ ")
        return generated_sequence
    
    def _predict_next_token_unified(
        self, 
        current_embeddings: torch.Tensor,
        in_image_generation: bool,
        patches_generated: int,
        max_patches: Optional[int],
        do_sample: bool, 
        temperature: float,
        past_key_values: NaiveCache,
        kv_lens: List[int],
        kv_indexes: torch.Tensor
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªtokené¢„æµ‹ï¼Œå®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        å…³é”®ç‚¹ï¼š
        1. å¦‚æœä¸åœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œé¢„æµ‹æ–‡æœ¬tokenæˆ–ç‰¹æ®Štokenï¼ˆåŒ…æ‹¬<vision_start>ï¼‰
        2. å¦‚æœåœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œç”Ÿæˆå›¾åƒpatchç›´åˆ°å®Œæˆï¼Œç„¶åé¢„æµ‹<vision_end>
        
        Args:
            current_embeddings: å½“å‰åºåˆ—çš„embeddings
            in_image_generation: æ˜¯å¦åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­
            patches_generated: å½“å‰å›¾åƒå·²ç”Ÿæˆçš„patchæ•°
            max_patches: å½“å‰å›¾åƒçš„æœ€å¤§patchæ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        # æ„å»ºä½ç½®ID
        seq_len = current_embeddings.size(0)
        position_ids = torch.arange(seq_len, device=self.device)
        
        # LLMå‰å‘ä¼ æ’­
        output = self.model.language_model(
            packed_query_sequence=current_embeddings.unsqueeze(0),
            query_lens=torch.tensor([seq_len], device=self.device),
            packed_query_position_ids=position_ids,
            packed_query_indexes=torch.arange(seq_len, device=self.device),
            past_key_values=past_key_values,
            key_values_lens=torch.tensor(kv_lens, device=self.device),
            packed_key_value_indexes=kv_indexes,
            update_past_key_values=True,
            is_causal=True,
            mode="und"
        )
        
        # è·å–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€
        # æ ¹æ®qwen2_navit.pyçš„BaseNavitOutputWithPastï¼Œéœ€è¦è·å–packed_query_sequence
        hidden_states = output.packed_query_sequence
            
        last_hidden_state = hidden_states[0, -1, :]
        
        if not in_image_generation:
            # ä¸åœ¨å›¾åƒç”Ÿæˆä¸­ï¼šé¢„æµ‹æ–‡æœ¬tokenæˆ–ç‰¹æ®Štoken
            logits = self.model.language_model.lm_head(last_hidden_state.unsqueeze(0))
            
            if do_sample:
                import torch.nn.functional as F
                probs = F.softmax(logits / temperature, dim=-1)
                next_token = torch.multinomial(probs.squeeze(0), num_samples=1)
            else:
                next_token = torch.argmax(logits, dim=-1)
            
            token_id = next_token.item()
            
            # åˆ¤æ–­tokenç±»å‹
            if token_id == self.start_of_image:
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            elif token_id == self.end_of_image:
                # ä¸åº”è¯¥åœ¨éå›¾åƒç”Ÿæˆæ—¶é¢„æµ‹åˆ°end_of_image
                # ä½†ä¸ºäº†é²æ£’æ€§è¿˜æ˜¯å¤„ç†
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            elif token_id == self.eos_token_id:
                return {
                    'token_id': token_id,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
            else:
                return {
                    'token_id': token_id,
                    'token_type': 'text',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
        else:
            # åœ¨å›¾åƒç”Ÿæˆä¸­
            if patches_generated < max_patches:
                # è¿˜éœ€è¦ç”Ÿæˆæ›´å¤špatches
                return {
                    'token_type': 'image_patch',
                    'hidden_state': last_hidden_state
                }
            else:
                # å·²ç”Ÿæˆæ‰€æœ‰patchesï¼Œåº”è¯¥é¢„æµ‹<vision_end>
                # å¼ºåˆ¶é¢„æµ‹<vision_end>æˆ–è®©æ¨¡å‹è‡ªå·±å†³å®š
                logits = self.model.language_model.lm_head(last_hidden_state.unsqueeze(0))
                
                # è¿™é‡Œå¯ä»¥é€‰æ‹©ï¼š
                # 1. å¼ºåˆ¶è¾“å‡º<vision_end>
                # 2. è®©æ¨¡å‹è‡ªå·±é¢„æµ‹ï¼ˆå¯èƒ½ç»§ç»­ç”Ÿæˆæˆ–ç»“æŸï¼‰
                
                # æ–¹æ¡ˆ1ï¼šå¼ºåˆ¶è¾“å‡ºï¼ˆæ›´ç¨³å®šï¼‰
                return {
                    'token_id': self.end_of_image,
                    'token_type': 'special',
                    'logits': logits,
                    'hidden_state': last_hidden_state
                }
                


    def _generate_image_with_helper(
        self,
        image_shape: Tuple[int, int],
        gen_context: Dict[str, Any],
        cfg_text_context: Dict[str, Any],
        cfg_img_context: Dict[str, Any],
        cfg_text_scale: float,
        cfg_img_scale: float,
        num_timesteps: int,
        timestep_shift: float,
        cfg_interval: Union[Tuple[float, float], List[float]],
        cfg_renorm_min: float,
        cfg_renorm_type: str,
        enable_taylorseer: bool,
    ) -> Image.Image:
        """å¤ç”¨å®˜æ–¹ InterleaveInferencer çš„æ•´å›¾ç”Ÿæˆèƒ½åŠ›ã€‚"""
        if isinstance(cfg_interval, list):
            cfg_interval = tuple(cfg_interval)

        autocast_ctx = (
            torch.autocast(device_type=self.device.type, dtype=torch.bfloat16)
            if self.device.type == "cuda" else nullcontext()
        )

        with autocast_ctx:
            image = self.interleave_helper.gen_image(
                image_shape=image_shape,
                gen_context=gen_context,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                cfg_text_precontext=deepcopy(cfg_text_context),
                cfg_img_precontext=deepcopy(cfg_img_context),
                cfg_interval=cfg_interval,
                cfg_renorm_min=cfg_renorm_min,
                cfg_renorm_type=cfg_renorm_type,
                num_timesteps=num_timesteps,
                timestep_shift=timestep_shift,
                enable_taylorseer=enable_taylorseer,
            )

        return image

    def _generate_image_patch_unified(
        self, 
        current_embeddings: torch.Tensor, 
        image_shape: tuple, 
        patch_index: int,
        cfg_text_scale: float,
        cfg_img_scale: float,
        past_key_values: NaiveCache,
        kv_lens: List[int],
        kv_indexes: torch.Tensor,
        num_flow_steps: int = 10
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•ä¸ªå›¾åƒpatchï¼Œä½¿ç”¨Flow Matchingï¼Œå¯¹åº”è®­ç»ƒæ—¶çš„é€»è¾‘
        
        ä¸è®­ç»ƒçš„å¯¹åº”å…³ç³»ï¼š
        1. è®­ç»ƒæ—¶ï¼šç»™å®šnoisy patchï¼Œé¢„æµ‹velocity (noise - clean)
        2. æ¨ç†æ—¶ï¼šä»çº¯å™ªå£°å¼€å§‹ï¼Œé€šè¿‡é¢„æµ‹çš„velocityé€æ­¥å»å™ª
        
        Args:
            current_embeddings: å½“å‰åºåˆ—çš„embeddings
            image_shape: å›¾åƒå°ºå¯¸
            patch_index: å½“å‰patchçš„ç´¢å¼•
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦  
            num_flow_steps: Flow Matchingçš„å»å™ªæ­¥æ•°
            
        Returns:
            åŒ…å«patchä¿¡æ¯çš„å­—å…¸
        """
        # è®¡ç®—patchçš„ä½ç½®å’Œå°ºå¯¸ä¿¡æ¯
        H, W = image_shape
        h = H // self.model.latent_downsample
        w = W // self.model.latent_downsample
        
        # ç”Ÿæˆä½ç½®ID
        patch_position_ids = self.model.get_flattened_position_ids(
            h * self.model.latent_downsample, w * self.model.latent_downsample,
            self.model.latent_downsample,
            max_num_patches_per_side=self.model.max_latent_size
        ).to(self.device)
        
        current_patch_pos_id = patch_position_ids[patch_index:patch_index+1]
        
        # åˆå§‹åŒ–ï¼šä»çº¯å™ªå£°å¼€å§‹
        patch_dim = self.model.latent_patch_size ** 2 * self.model.latent_channel
        x_t = torch.randn(1, patch_dim, device=self.device)  # çº¯å™ªå£°
        
        # Flow Matchingå»å™ªè¿‡ç¨‹
        timesteps = torch.linspace(1, 0, num_flow_steps, device=self.device)
        for t in timesteps:
            # å‡†å¤‡æ—¶é—´æ­¥
            timestep = torch.tensor([t], device=self.device)
            timestep_processed = torch.sigmoid(timestep)
            timestep_processed = self.model.timestep_shift * timestep_processed / (1 + (self.model.timestep_shift - 1) * timestep_processed)
            
            # æ„å»ºå½“å‰patchçš„embedding
            timestep_embed = self.model.time_embedder(timestep)
            latent_pos_embed = self.model.latent_pos_embed(current_patch_pos_id)
            patch_embedding = self.model.vae2llm(x_t) + timestep_embed + latent_pos_embed
            
            # å°†patch embeddingæ·»åŠ åˆ°å½“å‰åºåˆ—æœ«å°¾ï¼ˆä¸´æ—¶ï¼‰
            temp_embeddings = torch.cat([current_embeddings, patch_embedding.squeeze(0)], dim=0)
            
            # æ„å»ºä½ç½®ID
            seq_len = temp_embeddings.size(0)
            position_ids = torch.arange(seq_len, device=self.device)
            
            # LLMå‰å‘ä¼ æ’­
            output = self.model.language_model(
                packed_query_sequence=temp_embeddings.unsqueeze(0),
                query_lens=torch.tensor([seq_len], device=self.device),
                packed_query_position_ids=position_ids,
                packed_query_indexes=torch.arange(seq_len, device=self.device),
                past_key_values=past_key_values,
                key_values_lens=torch.tensor(kv_lens, device=self.device),
                packed_key_value_indexes=kv_indexes,
                update_past_key_values=True,
                is_causal=True,
                mode="und"
            )
            
            # è·å–patchä½ç½®çš„éšè—çŠ¶æ€
            # å¤„ç†ä¸åŒçš„æ¨¡å‹è¾“å‡ºæ ¼å¼
            if hasattr(output, 'packed_query_sequence'):
                # BaseNavitOutputWithPast æ ¼å¼
                hidden_states = output.packed_query_sequence
            elif hasattr(output, 'last_hidden_state'):
                # å¦‚æœæ˜¯ä¸€ä¸ªåŒ…å« last_hidden_state å±æ€§çš„å¯¹è±¡
                hidden_states = output.last_hidden_state
            elif isinstance(output, tuple):
                # å¦‚æœæ˜¯å…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ hidden states
                hidden_states = output[0]
            else:
                # å¦åˆ™ç›´æ¥ä½¿ç”¨
                hidden_states = output
            
            patch_hidden_state = hidden_states[0, -1, :]
            
            # é¢„æµ‹velocity
            v_pred = self.model.llm2vae(patch_hidden_state.unsqueeze(0))
            
            # CFGï¼ˆå¦‚æœéœ€è¦ï¼‰
            if cfg_text_scale > 1.0 or cfg_img_scale > 1.0:
                # TODO: å®ç°CFGé€»è¾‘
                pass
            
            # æ›´æ–°x_tï¼ˆå‘clean dataæ–¹å‘ç§»åŠ¨ï¼‰
            dt = timesteps[1] - timesteps[0] if len(timesteps) > 1 else t
            x_t = x_t - v_pred * dt  # velocityæŒ‡å‘noiseåˆ°dataçš„æ–¹å‘
        
        # æœ€ç»ˆçš„clean latent
        clean_latent = x_t
        
        # å‡†å¤‡æœ€ç»ˆçš„embeddingï¼ˆä½¿ç”¨clean latentï¼Œtimestep=0ï¼‰
        timestep_final = torch.zeros(1, device=self.device)
        timestep_embed_final = self.model.time_embedder(timestep_final)
        latent_pos_embed_final = self.model.latent_pos_embed(current_patch_pos_id)
        patch_embedding_final = self.model.vae2llm(clean_latent) + timestep_embed_final + latent_pos_embed_final
        
        return {
            'embedding': patch_embedding_final.squeeze(0),  # ç§»é™¤batchç»´åº¦
            'latent': clean_latent.squeeze(0),
            'position_id': current_patch_pos_id,
            'timestep': timestep_final,
            'patch_index': patch_index
        }
    
    def _finalize_image_generation(
        self, 
        image_patches: List[Dict[str, Any]], 
        image_shape: tuple,
        cfg_text_scale: float,
        cfg_img_scale: float,
        num_timesteps: int,
        timestep_shift: float
    ) -> Image.Image:
        """
        å®Œæˆå›¾åƒç”Ÿæˆï¼Œå°†patchesåˆæˆä¸ºå®Œæ•´å›¾åƒ
        """
        print(f"ğŸ–¼ï¸  åˆæˆå›¾åƒï¼Œå…± {len(image_patches)} ä¸ªpatches")
        
        # æ”¶é›†æ‰€æœ‰patchçš„latent
        patch_latents = []
        for patch_data in image_patches:
            patch_latents.append(patch_data['latent'])
        
        if not patch_latents:
            # å¦‚æœæ²¡æœ‰patchæ•°æ®ï¼Œç”Ÿæˆéšæœºå›¾åƒ
            H, W = image_shape
            h = H // self.model.latent_downsample
            w = W // self.model.latent_downsample
            patch_dim = self.model.latent_patch_size ** 2 * self.model.latent_channel
            total_patches = h * w
            
            combined_latent = torch.randn(total_patches, patch_dim, device=self.device)
        else:
            combined_latent = torch.stack(patch_latents, dim=0)
        
        # è§£ç ä¸ºå›¾åƒ
        decoded_image = self._decode_patches_to_image(combined_latent, image_shape)
        
        return decoded_image
    
    def _decode_patches_to_image(
        self, 
        latent_patches: torch.Tensor, 
        image_shape: tuple
    ) -> Image.Image:
        """
        å°†latent patchesè§£ç ä¸ºå®Œæ•´å›¾åƒ
        """
        H, W = image_shape
        h = H // self.model.latent_downsample
        w = W // self.model.latent_downsample
        
        # é‡å¡‘ä¸ºå›¾åƒæ ¼å¼
        latent = latent_patches.reshape(
            1, h, w, 
            self.model.latent_patch_size, 
            self.model.latent_patch_size, 
            self.model.latent_channel
        )
        latent = torch.einsum("nhwpqc->nchpwq", latent)
        latent = latent.reshape(
            1, self.model.latent_channel, 
            h * self.model.latent_patch_size, 
            w * self.model.latent_patch_size
        )
        
        # VAEè§£ç 
        with torch.no_grad():
            image = self.vae_model.decode(latent)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = (image * 0.5 + 0.5).clamp(0, 1)[0].permute(1, 2, 0) * 255
        image = Image.fromarray(image.to(torch.uint8).cpu().numpy())
        
        return image
    
    def _parse_generated_sequence(
        self, 
        generated_sequence: List[Dict[str, Any]]
    ) -> List[Union[str, Image.Image]]:
        """
        è§£æç”Ÿæˆåºåˆ—ï¼Œç»„åˆæ–‡æœ¬å’Œå›¾åƒè¾“å‡º
        """
        output_results = []
        text_buffer = []
        
        for item in generated_sequence:
            item_type = item['type']
            
            if item_type == 'text_token':
                text_buffer.append(item['content'])
                
            elif item_type == 'special_token':
                # ç‰¹æ®Štokenä¸åŒ…å«åœ¨æ–‡æœ¬è¾“å‡ºä¸­ï¼Œä½†å¯ä»¥ä½œä¸ºåˆ†éš”ç¬¦
                if text_buffer and item['token_name'] in ['start_of_image']:
                    # åœ¨å›¾åƒå¼€å§‹å‰ï¼Œè¾“å‡ºç´¯ç§¯çš„æ–‡æœ¬
                    try:
                        text = self.tokenizer.decode(text_buffer, skip_special_tokens=True)
                        if text.strip():
                            output_results.append(text.strip())
                    except:
                        output_results.append("[æ–‡æœ¬è§£ç å¤±è´¥]")
                    text_buffer = []
                    
            elif item_type == 'image':
                # æ·»åŠ ç”Ÿæˆçš„å›¾åƒ
                output_results.append(item['content'])
                
            elif item_type == 'image_patch':
                # patchä¸ç›´æ¥è¾“å‡ºï¼Œç”±_finalize_image_generationå¤„ç†
                pass
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if text_buffer:
            try:
                text = self.tokenizer.decode(text_buffer, skip_special_tokens=True)
                if text.strip():
                    output_results.append(text.strip())
            except:
                output_results.append("[æ–‡æœ¬è§£ç å¤±è´¥]")
        
        return output_results


def load_bagel_model_for_inference(
    model_path: str,
    mode: int = 1,  # 1: æ­£å¸¸æ¨¡å¼, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–
    device: str = "cuda"
) -> Tuple[Bagel, Any, Qwen2Tokenizer, ImageTransform, ImageTransform, Dict[str, int]]:
    """
    æ­£ç¡®åŠ è½½BAGELæ¨¡å‹ç”¨äºæ¨ç†
    å‚è€ƒapp.pyå’Œinferencer.pyçš„åŠ è½½æ–¹å¼
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        mode: åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)
        device: è®¾å¤‡
        
    Returns:
        (model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids)
    """
    print(f"ğŸ“¦ å¼€å§‹åŠ è½½BAGELæ¨¡å‹ï¼Œè·¯å¾„: {model_path}")
    print(f"ğŸ”§ åŠ è½½æ¨¡å¼: {mode} (1=æ­£å¸¸, 2=NF4é‡åŒ–, 3=INT8é‡åŒ–)")
    
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, "llm_config.json"))
    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = "Qwen2MoTDecoderLayer"
    
    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, "vit_config.json"))
    vit_config.rope = False
    vit_config.num_hidden_layers -= 1
    
    # 2. åŠ è½½VAEæ¨¡å‹
    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, "ae.safetensors"))
    print("âœ… VAEæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. åˆ›å»ºBAGELé…ç½®
    config = BagelConfig(
        visual_gen=True,
        visual_und=True,
        llm_config=llm_config, 
        vit_config=vit_config,
        vae_config=vae_config,
        vit_max_num_patch_per_side=70,
        connector_act='gelu_pytorch_tanh',
        latent_patch_size=2,
        max_latent_size=64,
    )
    
    # 4. åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨init_empty_weightsé¿å…å†…å­˜é—®é¢˜ï¼‰
    with init_empty_weights():
        language_model = Qwen2ForCausalLM(llm_config)
        vit_model = SiglipVisionModel(vit_config)
        model = Bagel(language_model, vit_model, config)
        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)
    
    # 5. åŠ è½½tokenizerå’Œç‰¹æ®Štokens
    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)
    print("âœ… Tokenizerå’Œç‰¹æ®ŠtokensåŠ è½½å®Œæˆ")
    
    # 6. åˆ›å»ºå›¾åƒå˜æ¢
    vae_transform = ImageTransform(1024, 512, 16)
    vit_transform = ImageTransform(980, 224, 14)
    print("âœ… å›¾åƒå˜æ¢åˆ›å»ºå®Œæˆ")
    
    # 7. è®¾ç½®è®¾å¤‡æ˜ å°„
    device_map = infer_auto_device_map(
        model,
        max_memory={i: "80GiB" for i in range(torch.cuda.device_count())},
        no_split_module_classes=["Bagel", "Qwen2MoTDecoderLayer"],
    )
    
    # ç¡®ä¿ç›¸å…³æ¨¡å—åœ¨åŒä¸€è®¾å¤‡ä¸Š
    same_device_modules = [
        'language_model.model.embed_tokens',
        'time_embedder',
        'latent_pos_embed',
        'vae2llm',
        'llm2vae',
        'connector',
        'vit_pos_embed'
    ]
    
    fallback_device = "cuda:0" if torch.cuda.is_available() else "cpu"
    # ä»è‡ªåŠ¨æ¨ç†çš„ device_map ä¸­æå–å·²æœ‰çš„ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼Œä½œä¸ºå…œåº•
    inferred_devices = [dev for dev in device_map.values() if dev is not None]
    if inferred_devices:
        fallback_device = inferred_devices[0]
    def _select_first_device() -> str:
        first = device_map.get(same_device_modules[0])
        if first is None:
            return fallback_device
        return first
    if torch.cuda.device_count() <= 1:
        first_device = device_map.get(same_device_modules[0], fallback_device)
        for module_name in same_device_modules:
            device_map[module_name] = first_device
    else:
        first_device = _select_first_device()
        for module_name in same_device_modules:
            device_map[module_name] = first_device

    # å°†VAEæ¨¡å‹ç§»åŠ¨åˆ°ä¸»è¦è®¾å¤‡ï¼Œé¿å…åç»­CPU/GPUä¸ä¸€è‡´
    vae_device = first_device if isinstance(first_device, str) else fallback_device
    vae_model = vae_model.to(vae_device)
    # 8. æ ¹æ®æ¨¡å¼åŠ è½½æƒé‡
    if mode == 1:  # æ­£å¸¸æ¨¡å¼
        model = load_checkpoint_and_dispatch(
            model, 
            checkpoint=os.path.join(model_path, "ema.safetensors"), 
            device_map=device_map,
            offload_folder="offload",
            dtype=torch.bfloat16,
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (æ­£å¸¸æ¨¡å¼)")
        
    elif mode == 2:  # NF4é‡åŒ–
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_4bit=True, 
            bnb_4bit_compute_dtype=torch.bfloat16, 
            bnb_4bit_use_double_quant=False, 
            bnb_4bit_quant_type="nf4"
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (NF4é‡åŒ–æ¨¡å¼)")
        
    elif mode == 3:  # INT8é‡åŒ–
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_8bit=True, 
            torch_dtype=torch.float32
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ (INT8é‡åŒ–æ¨¡å¼)")
        
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„åŠ è½½æ¨¡å¼: {mode}")
    
    print("ğŸ‰ BAGELæ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids


class UnifiedImageEditingInference:
    """ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“"""
    
    def __init__(
        self,
        model_path: str,
        mode: int = 1,
        device: str = "cuda"
    ):
        """
        åˆå§‹åŒ–å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“
        
        Args:
            model_path: BAGELæ¨¡å‹è·¯å¾„
            mode: åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)
            device: è®¾å¤‡
        """
        self.device = device
        self.model_path = model_path
        
        # åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
        (self.model, self.vae_model, self.tokenizer, 
         self.vae_transform, self.vit_transform, self.new_token_ids) = load_bagel_model_for_inference(
            model_path=model_path,
            mode=mode,
            device=device
        )

        # ç¡®ä¿VAEä¸ä¸»æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡
        model_device = next(self.model.parameters()).device
        self.vae_model = self.vae_model.to(model_device)
        
        # åˆ›å»ºæ¨ç†å™¨ï¼ˆä¿ç•™åŸæœ‰çš„å…¼å®¹æ€§ï¼‰
        self.inferencer = InterleaveInferencer(
            model=self.model,
            vae_model=self.vae_model,
            tokenizer=self.tokenizer,
            vae_transform=self.vae_transform,
            vit_transform=self.vit_transform,
            new_token_ids=self.new_token_ids
        )
        
        # åˆ›å»ºç»Ÿä¸€è‡ªå›å½’æ¨ç†å™¨ï¼ˆæ–°å¢ï¼‰
        self.unified_inferencer = UnifiedAutoregressiveInferencer(
            model=self.model,
            vae_model=self.vae_model,
            tokenizer=self.tokenizer,
            vae_transform=self.vae_transform,
            vit_transform=self.vit_transform,
            new_token_ids=self.new_token_ids
        )
        
        print("ğŸš€ å›¾åƒç¼–è¾‘æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
    
    def edit_image(
        self,
        image_path: str,
        edit_prompt: str,
        think: bool = True,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        å›¾åƒç¼–è¾‘çš„ä¸»è¦æ¥å£
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            think: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
        """
        print(f"ğŸ–¼ï¸  å¼€å§‹å›¾åƒç¼–è¾‘")
        print(f"ğŸ“¸ è¾“å…¥å›¾åƒ: {image_path}")
        print(f"âœï¸  ç¼–è¾‘æç¤º: {edit_prompt}")
        
        # åŠ è½½è¾“å…¥å›¾åƒ
        try:
            input_image = Image.open(image_path).convert('RGB')
            print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {input_image.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return [f"å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        # æ„å»ºè¾“å…¥åˆ—è¡¨
        input_lists = [input_image, edit_prompt]
        
        # æ‰§è¡Œæ¨ç†
        try:
            results = self.inferencer.interleave_inference(
                input_lists=input_lists,
                think=think,
                understanding_output=False,  # ç¼–è¾‘æ¨¡å¼ï¼Œä¸æ˜¯ç†è§£æ¨¡å¼
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
                image_shapes=image_shapes,
                **kwargs
            )
            
            print(f"âœ… ç¼–è¾‘å®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ å›¾åƒç¼–è¾‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"å›¾åƒç¼–è¾‘å¤±è´¥: {e}"]
    
    def unified_edit_image(
        self,
        image_path: str,
        edit_prompt: str,
        use_autoregressive: bool = True,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 0.8,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        ç»Ÿä¸€å›¾åƒç¼–è¾‘æ¥å£ - æ”¯æŒçœŸæ­£çš„è‡ªå›å½’äº¤é”™æ¨ç†
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_prompt: ç¼–è¾‘æç¤ºè¯
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼ï¼ˆå¯¹åº”è®­ç»ƒï¼‰
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒï¼‰
        """
        print(f"ğŸ¨ å¼€å§‹ç»Ÿä¸€å›¾åƒç¼–è¾‘")
        print(f"ğŸ“¸ è¾“å…¥å›¾åƒ: {image_path}")
        print(f"âœï¸  ç¼–è¾‘æç¤º: {edit_prompt}")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        # åŠ è½½è¾“å…¥å›¾åƒ
        try:
            input_image = Image.open(image_path).convert('RGB')
            print(f"âœ… å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {input_image.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return [f"å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        try:
            if use_autoregressive:
                # ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†ï¼ˆå¯¹åº”è®­ç»ƒé€»è¾‘ï¼‰
                print("ğŸš€ ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†æ¨¡å¼")
                results = self.unified_inferencer.unified_autoregressive_inference(
                    input_text=edit_prompt,
                    input_image=input_image,
                    max_length=max_length,
                    do_sample=do_sample,
                    temperature=temperature,
                    image_shapes=image_shapes,
                    cfg_text_scale=cfg_text_scale,
                    cfg_img_scale=cfg_img_scale,
                    num_timesteps=num_timesteps,
                    timestep_shift=3.0,  # æ·»åŠ é»˜è®¤å€¼
                    **kwargs
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†ï¼ˆå…¼å®¹æ€§ï¼‰
                print("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†æ¨¡å¼")
                input_lists = [input_image, edit_prompt]
                results = self.inferencer.interleave_inference(
                    input_lists=input_lists,
                    understanding_output=False,
                    cfg_text_scale=cfg_text_scale,
                    cfg_img_scale=cfg_img_scale,
                    num_timesteps=num_timesteps,
                    image_shapes=image_shapes,
                    **kwargs
                )
            
            print(f"âœ… å›¾åƒç¼–è¾‘å®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ ç»Ÿä¸€å›¾åƒç¼–è¾‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"ç»Ÿä¸€å›¾åƒç¼–è¾‘å¤±è´¥: {e}"]
    
    def autoregressive_multi_modal_generation(
        self,
        prompt: str,
        input_image: Optional[str] = None,
        force_image_generation: bool = False,
        max_length: int = 500,
        do_sample: bool = True,
        temperature: float = 0.8,
        cfg_text_scale: float = 3.0,
        cfg_img_scale: float = 1.5,
        num_timesteps: int = 50,
        image_shapes: Tuple[int, int] = (1024, 1024),
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ - å®Œå…¨å¯¹åº”è®­ç»ƒæ—¶çš„ç»Ÿä¸€åºåˆ—å»ºæ¨¡
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            input_image: å¯é€‰çš„è¾“å…¥å›¾åƒè·¯å¾„
            force_image_generation: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå›¾åƒ
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: é‡‡æ ·æ¸©åº¦
            cfg_text_scale: æ–‡æœ¬CFGå¼ºåº¦
            cfg_img_scale: å›¾åƒCFGå¼ºåº¦
            num_timesteps: å»å™ªæ­¥æ•°
            image_shapes: è¾“å‡ºå›¾åƒå°ºå¯¸
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨ï¼ˆæ–‡æœ¬å’Œå›¾åƒäº¤é”™ï¼‰
        """
        print(f"ğŸ¯ å¼€å§‹è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ")
        print(f"ğŸ“ è¾“å…¥æç¤º: {prompt}")
        print(f"ğŸ–¼ï¸  è¾“å…¥å›¾åƒ: {input_image if input_image else 'None'}")
        print(f"ğŸ”§ å¼ºåˆ¶å›¾åƒç”Ÿæˆ: {force_image_generation}")
        
        # å¤„ç†è¾“å…¥å›¾åƒ
        image_obj = None
        if input_image:
            try:
                image_obj = Image.open(input_image).convert('RGB')
                print(f"âœ… è¾“å…¥å›¾åƒåŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {image_obj.size}")
            except Exception as e:
                print(f"âŒ è¾“å…¥å›¾åƒåŠ è½½å¤±è´¥: {e}")
                return [f"è¾“å…¥å›¾åƒåŠ è½½å¤±è´¥: {e}"]
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        if force_image_generation and '<|vision_start|>' not in prompt:
            # å¼ºåˆ¶æ·»åŠ å›¾åƒç”Ÿæˆtoken
            enhanced_prompt = f"{prompt} <|vision_start|> <|vision_end|>"
            print(f"ğŸ”§ å¼ºåˆ¶å›¾åƒç”Ÿæˆï¼Œå¢å¼ºæç¤º: {enhanced_prompt}")
        else:
            enhanced_prompt = prompt
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†
            results = self.unified_inferencer.unified_autoregressive_inference(
                input_text=enhanced_prompt,
                input_image=image_obj,
                max_length=max_length,
                do_sample=do_sample,
                temperature=temperature,
                image_shapes=image_shapes,
                cfg_text_scale=cfg_text_scale,
                cfg_img_scale=cfg_img_scale,
                num_timesteps=num_timesteps,
                **kwargs
            )
            
            print(f"âœ… è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            print(f"âŒ è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {e}"]
    
    def batch_edit_images(
        self,
        image_paths: List[str],
        edit_prompts: List[str],
        output_dir: str = "edited_images",
        use_autoregressive: bool = True,
        **kwargs
    ) -> Dict[str, List[Union[str, Image.Image]]]:
        """
        æ‰¹é‡å›¾åƒç¼–è¾‘
        
        Args:
            image_paths: è¾“å…¥å›¾åƒè·¯å¾„åˆ—è¡¨
            edit_prompts: ç¼–è¾‘æç¤ºè¯åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘ç»“æœå­—å…¸
        """
        print(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å›¾åƒç¼–è¾‘ï¼Œå…± {len(image_paths)} å¼ å›¾åƒ")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        os.makedirs(output_dir, exist_ok=True)
        results = {}
        
        for i, (image_path, edit_prompt) in enumerate(zip(image_paths, edit_prompts)):
            print(f"\nğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(image_paths)} å¼ å›¾åƒ")
            
            # å•å¼ å›¾åƒç¼–è¾‘ - ä½¿ç”¨ç»Ÿä¸€æ¥å£
            if use_autoregressive:
                edit_results = self.unified_edit_image(
                    image_path=image_path,
                    edit_prompt=edit_prompt,
                    use_autoregressive=True,
                    **kwargs
                )
            else:
                edit_results = self.edit_image(
                    image_path=image_path,
                    edit_prompt=edit_prompt,
                    **kwargs
                )
            
            # ä¿å­˜ç»“æœ
            results[f"image_{i+1}"] = edit_results
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾åƒ
            for j, result in enumerate(edit_results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(output_dir, f"image_{i+1}_result_{j}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(output_dir, f"image_{i+1}_text_{j}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}")
        
        print(f"ğŸ‰ æ‰¹é‡ç¼–è¾‘å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")
        return results
    
    def multi_step_edit(
        self,
        image_path: str,
        edit_steps: List[str],
        output_dir: str = "multi_step_edit",
        use_autoregressive: bool = True,
        **kwargs
    ) -> List[Union[str, Image.Image]]:
        """
        å¤šæ­¥éª¤å›¾åƒç¼–è¾‘
        
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            edit_steps: ç¼–è¾‘æ­¥éª¤åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            use_autoregressive: æ˜¯å¦ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨¡å¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ‰€æœ‰æ­¥éª¤çš„ç»“æœ
        """
        print(f"ğŸ¯ å¼€å§‹å¤šæ­¥éª¤å›¾åƒç¼–è¾‘ï¼Œå…± {len(edit_steps)} ä¸ªæ­¥éª¤")
        print(f"ğŸ”§ è‡ªå›å½’æ¨¡å¼: {use_autoregressive}")
        
        os.makedirs(output_dir, exist_ok=True)
        all_results = []
        current_image_path = image_path
        
        for i, edit_step in enumerate(edit_steps):
            print(f"\nğŸ”„ æ‰§è¡Œæ­¥éª¤ {i+1}/{len(edit_steps)}: {edit_step}")
            
            # æ‰§è¡Œå½“å‰æ­¥éª¤ - ä½¿ç”¨ç»Ÿä¸€æ¥å£
            if use_autoregressive:
                step_results = self.unified_edit_image(
                    image_path=current_image_path,
                    edit_prompt=edit_step,
                    use_autoregressive=True,
                    **kwargs
                )
            else:
                step_results = self.edit_image(
                    image_path=current_image_path,
                    edit_prompt=edit_step,
                    **kwargs
                )
            
            all_results.extend(step_results)
            
            # ä¿å­˜å½“å‰æ­¥éª¤ç»“æœ
            step_dir = os.path.join(output_dir, f"step_{i+1}")
            os.makedirs(step_dir, exist_ok=True)
            
            for j, result in enumerate(step_results):
                if isinstance(result, Image.Image):
                    result_path = os.path.join(step_dir, f"result_{j}.png")
                    result.save(result_path)
                    print(f"ğŸ’¾ æ­¥éª¤ {i+1} å›¾åƒå·²ä¿å­˜: {result_path}")
                    
                    # æ›´æ–°current_image_pathä¸ºæœ€æ–°ç”Ÿæˆçš„å›¾åƒï¼ˆç”¨äºä¸‹ä¸€æ­¥ï¼‰
                    current_image_path = result_path
                    
                elif isinstance(result, str):
                    result_path = os.path.join(step_dir, f"text_{j}.txt")
                    with open(result_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ æ­¥éª¤ {i+1} æ–‡æœ¬å·²ä¿å­˜: {result_path}")
        
        print(f"ğŸ‰ å¤šæ­¥éª¤ç¼–è¾‘å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {output_dir}")
        return all_results


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="BAGELç»Ÿä¸€å›¾åƒç¼–è¾‘æ¨ç†")
    parser.add_argument("--model_path", type=str, required=True, help="BAGELæ¨¡å‹è·¯å¾„")
    parser.add_argument("--image_path", type=str, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--edit_prompt", type=str, help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--prompt", type=str, help="é€šç”¨æç¤ºè¯ï¼ˆç”¨äºå¤šæ¨¡æ€ç”Ÿæˆï¼‰")
    parser.add_argument("--output_dir", type=str, default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--mode", type=int, default=1, help="æ¨¡å‹åŠ è½½æ¨¡å¼ (1: æ­£å¸¸, 2: NF4é‡åŒ–, 3: INT8é‡åŒ–)")
    
    # æ¨ç†æ¨¡å¼é€‰æ‹©
    parser.add_argument("--use_autoregressive", action="store_true", default=True, 
                      help="ä½¿ç”¨ç»Ÿä¸€è‡ªå›å½’æ¨ç†æ¨¡å¼ï¼ˆå¯¹åº”è®­ç»ƒï¼‰")
    parser.add_argument("--use_legacy", action="store_true", 
                      help="ä½¿ç”¨ä¼ ç»Ÿinterleaveæ¨ç†æ¨¡å¼ï¼ˆå…¼å®¹æ€§ï¼‰")
    parser.add_argument("--generation_mode", type=str, choices=["edit", "generate", "multi_modal"], 
                      default="edit", help="ç”Ÿæˆæ¨¡å¼ï¼šedit=å›¾åƒç¼–è¾‘, generate=çº¯ç”Ÿæˆ, multi_modal=å¤šæ¨¡æ€ç”Ÿæˆ")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_length", type=int, default=500, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--do_sample", action="store_true", default=True, help="æ˜¯å¦é‡‡æ ·")
    parser.add_argument("--temperature", type=float, default=0.8, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--force_image_generation", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆå›¾åƒ")
    
    parser.add_argument("--think", action="store_true", help="å¯ç”¨æ€è€ƒæ¨¡å¼")
    parser.add_argument("--cfg_text_scale", type=float, default=3.0, help="æ–‡æœ¬CFGå¼ºåº¦")
    parser.add_argument("--cfg_img_scale", type=float, default=1.5, help="å›¾åƒCFGå¼ºåº¦")
    parser.add_argument("--num_timesteps", type=int, default=50, help="å»å™ªæ­¥æ•°")
    parser.add_argument("--multi_step", nargs="+", help="å¤šæ­¥éª¤ç¼–è¾‘ï¼ˆæä¾›å¤šä¸ªç¼–è¾‘æ­¥éª¤ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    
    # å¤„ç†æ¨ç†æ¨¡å¼
    use_autoregressive = args.use_autoregressive and not args.use_legacy
    
    print("ğŸš€ åˆå§‹åŒ–BAGELç»Ÿä¸€æ¨ç†å¼•æ“...")
    print(f"ğŸ”§ æ¨ç†æ¨¡å¼: {'ç»Ÿä¸€è‡ªå›å½’' if use_autoregressive else 'ä¼ ç»Ÿäº¤é”™'}")
    print(f"ğŸ¯ ç”Ÿæˆæ¨¡å¼: {args.generation_mode}")
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“
        inference_engine = UnifiedImageEditingInference(
            model_path=args.model_path,
            mode=args.mode
        )
        
        os.makedirs(args.output_dir, exist_ok=True)
        
        # æ ¹æ®ç”Ÿæˆæ¨¡å¼æ‰§è¡Œä¸åŒçš„é€»è¾‘
        if args.generation_mode == "edit":
            # å›¾åƒç¼–è¾‘æ¨¡å¼
            if not args.image_path or not args.edit_prompt:
                print("âŒ å›¾åƒç¼–è¾‘æ¨¡å¼éœ€è¦ --image_path å’Œ --edit_prompt å‚æ•°")
                return
                
            if args.multi_step:
                # å¤šæ­¥éª¤ç¼–è¾‘
                print("ğŸ¯ æ‰§è¡Œå¤šæ­¥éª¤å›¾åƒç¼–è¾‘")
                results = inference_engine.multi_step_edit(
                    image_path=args.image_path,
                    edit_steps=args.multi_step,
                    output_dir=args.output_dir,
                    use_autoregressive=use_autoregressive,
                    max_length=args.max_length,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    cfg_text_scale=args.cfg_text_scale,
                    cfg_img_scale=args.cfg_img_scale,
                    num_timesteps=args.num_timesteps
                )
            else:
                # å•æ­¥ç¼–è¾‘
                print("âœï¸  æ‰§è¡Œå•æ­¥å›¾åƒç¼–è¾‘")
                results = inference_engine.unified_edit_image(
                    image_path=args.image_path,
                    edit_prompt=args.edit_prompt,
                    use_autoregressive=use_autoregressive,
                    max_length=args.max_length,
                    do_sample=args.do_sample,
                    temperature=args.temperature,
                    cfg_text_scale=args.cfg_text_scale,
                    cfg_img_scale=args.cfg_img_scale,
                    num_timesteps=args.num_timesteps
                )
                
                # ä¿å­˜ç»“æœ
                for i, result in enumerate(results):
                    if isinstance(result, Image.Image):
                        output_path = os.path.join(args.output_dir, f"edited_image_{i}.png")
                        result.save(output_path)
                        print(f"ğŸ’¾ ç¼–è¾‘ç»“æœå·²ä¿å­˜: {output_path}")
                    elif isinstance(result, str):
                        output_path = os.path.join(args.output_dir, f"text_result_{i}.txt")
                        with open(output_path, "w", encoding="utf-8") as f:
                            f.write(result)
                        print(f"ğŸ’¾ æ–‡æœ¬ç»“æœå·²ä¿å­˜: {output_path}")
                        
        elif args.generation_mode == "multi_modal":
            # å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼
            if not args.prompt:
                print("âŒ å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å¼éœ€è¦ --prompt å‚æ•°")
                return
                
            print("ğŸ¯ æ‰§è¡Œè‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆ")
            results = inference_engine.autoregressive_multi_modal_generation(
                prompt=args.prompt,
                input_image=args.image_path,
                force_image_generation=args.force_image_generation,
                max_length=args.max_length,
                do_sample=args.do_sample,
                temperature=args.temperature,
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.num_timesteps
            )
            
            # ä¿å­˜ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(args.output_dir, f"generated_image_{i}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ ç”Ÿæˆå›¾åƒå·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(args.output_dir, f"generated_text_{i}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ ç”Ÿæˆæ–‡æœ¬å·²ä¿å­˜: {output_path}")
                    
        elif args.generation_mode == "generate":
            # çº¯ç”Ÿæˆæ¨¡å¼ï¼ˆä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå¯¹æ¯”ï¼‰
            if not args.prompt:
                print("âŒ ç”Ÿæˆæ¨¡å¼éœ€è¦ --prompt å‚æ•°")
                return
                
            print("ğŸ¯ æ‰§è¡Œä¼ ç»Ÿæ¨ç†ç”Ÿæˆ")
            if args.image_path:
                input_image = Image.open(args.image_path).convert('RGB')
                input_lists = [input_image, args.prompt]
            else:
                input_lists = [args.prompt]
                
            results = inference_engine.inferencer.interleave_inference(
                input_lists=input_lists,
                think=args.think,
                understanding_output=False,
                cfg_text_scale=args.cfg_text_scale,
                cfg_img_scale=args.cfg_img_scale,
                num_timesteps=args.num_timesteps
            )
            
            # ä¿å­˜ç»“æœ
            for i, result in enumerate(results):
                if isinstance(result, Image.Image):
                    output_path = os.path.join(args.output_dir, f"traditional_image_{i}.png")
                    result.save(output_path)
                    print(f"ğŸ’¾ ä¼ ç»Ÿç”Ÿæˆå›¾åƒå·²ä¿å­˜: {output_path}")
                elif isinstance(result, str):
                    output_path = os.path.join(args.output_dir, f"traditional_text_{i}.txt")
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(result)
                    print(f"ğŸ’¾ ä¼ ç»Ÿç”Ÿæˆæ–‡æœ¬å·²ä¿å­˜: {output_path}")
        
        print("ğŸ‰ æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    
    main()
