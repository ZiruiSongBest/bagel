#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾åƒç¼–è¾‘Demo - å‚è€ƒapp.pyçš„edit_imageå®ç°æ–¹å¼
"""

import os
import torch
import random
import numpy as np
from PIL import Image
from accelerate import infer_auto_device_map, load_checkpoint_and_dispatch, init_empty_weights
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model

from data.data_utils import add_special_tokens, pil_img2rgb
from data.transforms import ImageTransform
from inferencer import InterleaveInferencer
from modeling.autoencoder import load_ae
from modeling.bagel import (
    BagelConfig, Bagel, Qwen2Config, Qwen2ForCausalLM,
    SiglipVisionConfig, SiglipVisionModel
)
from modeling.qwen2 import Qwen2Tokenizer


def set_seed(seed):
    """Set random seeds for reproducibility"""
    if seed > 0:
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    return seed


def load_bagel_model(model_path="models/BAGEL-7B-MoT", mode=1):
    """
    åŠ è½½BAGELæ¨¡å‹ï¼Œå‚è€ƒapp.pyçš„å®ç°
    """
    print(f"æ­£åœ¨åŠ è½½BAGELæ¨¡å‹ï¼Œè·¯å¾„: {model_path}")
    
    # åŠ è½½é…ç½®
    llm_config = Qwen2Config.from_json_file(os.path.join(model_path, "llm_config.json"))
    llm_config.qk_norm = True
    llm_config.tie_word_embeddings = False
    llm_config.layer_module = "Qwen2MoTDecoderLayer"

    vit_config = SiglipVisionConfig.from_json_file(os.path.join(model_path, "vit_config.json"))
    vit_config.rope = False
    vit_config.num_hidden_layers -= 1

    vae_model, vae_config = load_ae(local_path=os.path.join(model_path, "ae.safetensors"))

    config = BagelConfig(
        visual_gen=True,
        visual_und=True,
        llm_config=llm_config, 
        vit_config=vit_config,
        vae_config=vae_config,
        vit_max_num_patch_per_side=70,
        connector_act='gelu_pytorch_tanh',
        latent_patch_size=2,
        max_latent_size=64,
    )

    # åˆå§‹åŒ–ç©ºæ¨¡å‹
    with init_empty_weights():
        language_model = Qwen2ForCausalLM(llm_config)
        vit_model = SiglipVisionModel(vit_config)
        model = Bagel(language_model, vit_model, config)
        model.vit_model.vision_model.embeddings.convert_conv2d_to_linear(vit_config, meta=True)

    # åŠ è½½tokenizer
    tokenizer = Qwen2Tokenizer.from_pretrained(model_path)
    tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)

    # åˆ›å»ºtransforms
    vae_transform = ImageTransform(1024, 512, 16)
    vit_transform = ImageTransform(980, 224, 14)

    # æ¨¡å‹è®¾å¤‡æ˜ å°„
    device_map = infer_auto_device_map(
        model,
        max_memory={i: "80GiB" for i in range(torch.cuda.device_count())},
        no_split_module_classes=["Bagel", "Qwen2MoTDecoderLayer"],
    )

    same_device_modules = [
        'language_model.model.embed_tokens',
        'time_embedder',
        'latent_pos_embed',
        'vae2llm',
        'llm2vae',
        'connector',
        'vit_pos_embed'
    ]

    if torch.cuda.device_count() == 1:
        first_device = device_map.get(same_device_modules[0], "cuda:0")
        for k in same_device_modules:
            if k in device_map:
                device_map[k] = first_device
            else:
                device_map[k] = "cuda:0"
    else:
        first_device = device_map.get(same_device_modules[0])
        for k in same_device_modules:
            if k in device_map:
                device_map[k] = first_device

    # æ ¹æ®æ¨¡å¼åŠ è½½æ¨¡å‹æƒé‡
    if mode == 1:
        model = load_checkpoint_and_dispatch(
            model,
            checkpoint=os.path.join(model_path, "ema.safetensors"),
            device_map=device_map,
            offload_buffers=True,
            offload_folder="offload",
            dtype=torch.bfloat16,
            force_hooks=True,
        ).eval()
    elif mode == 2:  # NF4
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_4bit=True, 
            bnb_4bit_compute_dtype=torch.bfloat16, 
            bnb_4bit_use_double_quant=False, 
            bnb_4bit_quant_type="nf4"
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
    elif mode == 3:  # INT8
        bnb_quantization_config = BnbQuantizationConfig(
            load_in_8bit=True, 
            torch_dtype=torch.float32
        )
        model = load_and_quantize_model(
            model, 
            weights_location=os.path.join(model_path, "ema.safetensors"), 
            bnb_quantization_config=bnb_quantization_config,
            device_map=device_map,
            offload_folder="offload",
        ).eval()
    else:
        raise NotImplementedError(f"æ¨¡å¼ {mode} æœªå®ç°")

    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    return model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids


def edit_image(inferencer, image_path: str, prompt: str, show_thinking=False, 
               cfg_text_scale=4.0, cfg_img_scale=2.0, cfg_interval=0.0, 
               timestep_shift=3.0, num_timesteps=50, cfg_renorm_min=0.0, 
               cfg_renorm_type="text_channel", max_think_token_n=1024, 
               do_sample=False, text_temperature=0.3, seed=0):
    """
    å›¾åƒç¼–è¾‘å‡½æ•°ï¼Œå‚è€ƒapp.pyçš„edit_imageå®ç°
    
    Args:
        inferencer: InterleaveInferencerå®ä¾‹
        image_path: è¾“å…¥å›¾åƒè·¯å¾„
        prompt: ç¼–è¾‘æç¤º
        å…¶ä»–å‚æ•°ä¸app.pyä¸­edit_imageå‡½æ•°ç›¸åŒ
    
    Returns:
        tuple: (ç¼–è¾‘åçš„å›¾åƒ, æ€è€ƒè¿‡ç¨‹æ–‡æœ¬)
    """
    # è®¾ç½®éšæœºç§å­
    set_seed(seed)
    
    # åŠ è½½å›¾åƒ
    try:
        image = Image.open(image_path)
        print(f"æˆåŠŸåŠ è½½å›¾åƒ: {image_path}, å°ºå¯¸: {image.size}")
    except Exception as e:
        print(f"åŠ è½½å›¾åƒå¤±è´¥: {e}")
        return None, ""

    # è½¬æ¢ä¸ºnumpyæ•°ç»„å†è½¬å›PILï¼ˆå¦‚æœéœ€è¦ï¼‰
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)

    # è½¬æ¢ä¸ºRGBæ ¼å¼
    image = pil_img2rgb(image)
    
    # è®¾ç½®æ¨ç†è¶…å‚æ•°
    inference_hyper = dict(
        max_think_token_n=max_think_token_n if show_thinking else 1024,
        do_sample=do_sample if show_thinking else False,
        text_temperature=text_temperature if show_thinking else 0.3,
        cfg_text_scale=cfg_text_scale,
        cfg_img_scale=cfg_img_scale,
        cfg_interval=[cfg_interval, 1.0],  # End fixed at 1.0
        timestep_shift=timestep_shift,
        num_timesteps=num_timesteps,
        cfg_renorm_min=cfg_renorm_min,
        cfg_renorm_type=cfg_renorm_type,
    )
    
    print(f"å¼€å§‹å›¾åƒç¼–è¾‘...")
    print(f"æç¤ºè¯: {prompt}")
    print(f"æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹: {show_thinking}")
    print(f"æ¨ç†å‚æ•°: {inference_hyper}")
    
    # è°ƒç”¨inferencerè¿›è¡Œå›¾åƒç¼–è¾‘
    result = inferencer(image=image, text=prompt, think=show_thinking, **inference_hyper)
    
    return result["image"], result.get("text", "")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å›¾åƒç¼–è¾‘Demo")
    parser.add_argument("--model_path", type=str, default="models/BAGEL-7B-MoT",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--mode", type=int, default=1, choices=[1, 2, 3],
                       help="åŠ è½½æ¨¡å¼: 1=æ ‡å‡†, 2=NF4é‡åŒ–, 3=INT8é‡åŒ–")
    parser.add_argument("--image_path", type=str, 
                       default="/hdd_16T/Zirui/workspace/ATest/Bagel/assets/munchkin-cat-breed.jpg",
                       help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--prompt", type=str, 
                       default="å°†è¿™åªçŒ«å’ªå˜æˆä¸€åªç©¿ç€å°ç¤¼æœçš„ä¼˜é›…çŒ«å’ª",
                       help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--output_path", type=str, default="edited_cat.png",
                       help="è¾“å‡ºå›¾åƒè·¯å¾„")
    parser.add_argument("--show_thinking", action="store_true",
                       help="æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    # CFGå‚æ•°
    parser.add_argument("--cfg_text_scale", type=float, default=4.0,
                       help="æ–‡æœ¬CFGå¼ºåº¦")
    parser.add_argument("--cfg_img_scale", type=float, default=2.0,
                       help="å›¾åƒCFGå¼ºåº¦")
    parser.add_argument("--cfg_interval", type=float, default=0.0,
                       help="CFGåº”ç”¨é—´éš”èµ·å§‹å€¼")
    parser.add_argument("--timestep_shift", type=float, default=3.0,
                       help="æ—¶é—´æ­¥åç§»")
    parser.add_argument("--num_timesteps", type=int, default=50,
                       help="æ—¶é—´æ­¥æ•°")
    parser.add_argument("--cfg_renorm_type", type=str, default="text_channel",
                       choices=["global", "local", "text_channel"],
                       help="CFGé‡å½’ä¸€åŒ–ç±»å‹")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("å›¾åƒç¼–è¾‘Demo - åŸºäºBAGELæ¨¡å‹")
    print("=" * 60)
    
    try:
        # åŠ è½½æ¨¡å‹
        model, vae_model, tokenizer, vae_transform, vit_transform, new_token_ids = load_bagel_model(
            model_path=args.model_path,
            mode=args.mode
        )
        
        # åˆ›å»ºinferencer
        inferencer = InterleaveInferencer(
            model=model,
            vae_model=vae_model,
            tokenizer=tokenizer,
            vae_transform=vae_transform,
            vit_transform=vit_transform,
            new_token_ids=new_token_ids,
        )
        
        print("\nå¼€å§‹å›¾åƒç¼–è¾‘...")
        
        # æ‰§è¡Œå›¾åƒç¼–è¾‘
        edited_image, thinking_text = edit_image(
            inferencer=inferencer,
            image_path=args.image_path,
            prompt=args.prompt,
            show_thinking=args.show_thinking,
            cfg_text_scale=args.cfg_text_scale,
            cfg_img_scale=args.cfg_img_scale,
            cfg_interval=args.cfg_interval,
            timestep_shift=args.timestep_shift,
            num_timesteps=args.num_timesteps,
            cfg_renorm_type=args.cfg_renorm_type,
            seed=args.seed
        )
        
        # ä¿å­˜ç»“æœ
        if edited_image is not None:
            edited_image.save(args.output_path)
            print(f"\nâœ… ç¼–è¾‘æˆåŠŸï¼")
            print(f"åŸå§‹å›¾åƒ: {args.image_path}")
            print(f"ç¼–è¾‘æç¤º: {args.prompt}")
            print(f"è¾“å‡ºå›¾åƒ: {args.output_path}")
            print(f"å›¾åƒå°ºå¯¸: {edited_image.size}")
            
            if args.show_thinking and thinking_text:
                print(f"\nğŸ¤” æ€è€ƒè¿‡ç¨‹:")
                print(thinking_text)
        else:
            print("âŒ å›¾åƒç¼–è¾‘å¤±è´¥")
        
        print("\n" + "=" * 60)
        print("Demoè¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nè¯·æ£€æŸ¥:")
        print("1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. è¾“å…¥å›¾åƒæ˜¯å¦å­˜åœ¨")
        print("3. æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ")
        print("4. æ‰€æœ‰ä¾èµ–æ˜¯å¦å·²å®‰è£…")


if __name__ == "__main__":
    main()