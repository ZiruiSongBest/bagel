#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è¿‡æ‹Ÿåˆæƒé‡çš„è„šæœ¬

ä¸“é—¨ç”¨äºæµ‹è¯•è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®ï¼ŒéªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®å­¦ä¼šäº†æ–‡æœ¬+å›¾åƒçš„æ˜ å°„å…³ç³»
"""

import os
import sys
import torch
import json
from PIL import Image
from pathlib import Path
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, str(Path(__file__).parent))

from inference_unified_autoregressive import create_inference_engine
from training.unified_data_processor import UnifiedGenerationDataset
from data.transforms import ImageTransform
from data.data_utils import add_special_tokens
from modeling.qwen2 import Qwen2Tokenizer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_training_data_sample(data_path, num_samples=3):
    """åŠ è½½è®­ç»ƒæ•°æ®çš„æ ·æœ¬è¿›è¡Œæµ‹è¯•"""
    logger.info(f"ä»è®­ç»ƒæ•°æ®ä¸­åŠ è½½æ ·æœ¬: {data_path}")
    
    samples = []
    
    if os.path.exists(data_path):
        if data_path.endswith('.jsonl'):
            # JSONLæ ¼å¼
            with open(data_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= num_samples:
                        break
                    try:
                        data = json.loads(line.strip())
                        samples.append(data)
                    except Exception as e:
                        logger.warning(f"è·³è¿‡æ— æ•ˆè¡Œ {i}: {e}")
        elif data_path.endswith('.json'):
            # JSONæ ¼å¼
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    samples = data[:num_samples]
                else:
                    samples = [data]
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºä¸€äº›æµ‹è¯•æ ·æœ¬
        logger.warning(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        logger.info("ä½¿ç”¨æ¨¡æ‹Ÿçš„è®­ç»ƒæ•°æ®æ ¼å¼è¿›è¡Œæµ‹è¯•")
        samples = create_mock_training_samples()
    
    logger.info(f"åŠ è½½äº† {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return samples


def create_mock_training_samples():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„è®­ç»ƒæ•°æ®æ ·æœ¬ï¼ˆç¬¦åˆæ‚¨çš„è®­ç»ƒæ ¼å¼ï¼‰"""
    samples = [
        {
            "input_sequence": ["ç”»ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨èŠ±å›­é‡Œç©è€"],
            "target_sequence": ["è¿™æ˜¯ä¸€åªæ©˜è‰²çš„å°çŒ«ï¼Œåœ¨ç»¿è‰²çš„èŠ±å›­ä¸­å¿«ä¹åœ°ç©è€ã€‚", "<image>"],
            "input_types": ["text"],
            "target_types": ["text", "image"],
            "metadata": {"description": "æ–‡æœ¬åˆ°æ–‡æœ¬+å›¾åƒçš„ç”Ÿæˆ"}
        },
        {
            "input_sequence": ["æè¿°ä¸€ä¸‹æ˜¥å¤©çš„ç¾æ™¯ï¼Œç„¶åç”»å‡ºæ¥"],
            "target_sequence": ["æ˜¥å¤©æ¥äº†ï¼Œæ¨±èŠ±ç››å¼€ï¼Œç»¿è‰å¦‚èŒµï¼Œé˜³å…‰æ˜åªšã€‚", "<image>"],
            "input_types": ["text"], 
            "target_types": ["text", "image"],
            "metadata": {"description": "æè¿°+ç»˜ç”»çš„ç»„åˆä»»åŠ¡"}
        },
        {
            "input_sequence": ["åˆ›ä½œä¸€å¹…å±±æ°´ç”»"],
            "target_sequence": ["<image>"],
            "input_types": ["text"],
            "target_types": ["image"],
            "metadata": {"description": "çº¯æ–‡æœ¬åˆ°å›¾åƒ"}
        }
    ]
    return samples


def test_single_sample(engine, sample, sample_idx):
    """æµ‹è¯•å•ä¸ªè®­ç»ƒæ ·æœ¬"""
    logger.info(f"\n{'='*50}")
    logger.info(f"æµ‹è¯•æ ·æœ¬ {sample_idx + 1}")
    logger.info(f"{'='*50}")
    
    # æ„å»ºè¾“å…¥æ–‡æœ¬
    input_texts = sample.get("input_sequence", [])
    target_sequence = sample.get("target_sequence", [])
    
    if not input_texts:
        logger.warning("æ ·æœ¬ç¼ºå°‘è¾“å…¥åºåˆ—ï¼Œè·³è¿‡")
        return
    
    # åˆå¹¶è¾“å…¥æ–‡æœ¬
    input_text = " ".join([text for text in input_texts if isinstance(text, str)])
    logger.info(f"è¾“å…¥æ–‡æœ¬: {input_text}")
    
    # æ˜¾ç¤ºæœŸæœ›çš„è¾“å‡º
    logger.info("æœŸæœ›è¾“å‡º:")
    for i, target in enumerate(target_sequence):
        if isinstance(target, str):
            if target == "<image>":
                logger.info(f"  [{i}] å›¾åƒæ ‡è®°: <image>")
            else:
                logger.info(f"  [{i}] æ–‡æœ¬: {target}")
        else:
            logger.info(f"  [{i}] å…¶ä»–ç±»å‹: {type(target)}")
    
    try:
        # ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„ç”Ÿæˆé€»è¾‘
        # å¯¹äºè¿‡æ‹Ÿåˆæµ‹è¯•ï¼Œä½¿ç”¨è¾ƒä½çš„temperatureå’Œç¡®å®šæ€§é‡‡æ ·
        results = engine.autoregressive_generate(
            prompt=input_text,
            max_length=300,
            do_sample=True,  # ä½¿ç”¨é‡‡æ ·ä½†temperatureè¾ƒä½
            temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ¥è¿‘ç¡®å®šæ€§
            image_shapes=(512, 512),  # è¾ƒå°å°ºå¯¸åŠ å¿«æµ‹è¯•
            cfg_text_scale=3.0,
            cfg_img_scale=1.2,
            num_timesteps=25,  # å‡å°‘æ­¥æ•°åŠ å¿«æµ‹è¯•
            save_intermediate=True,
            output_dir=f"overfitting_test_sample_{sample_idx + 1}"
        )
        
        logger.info(f"å®é™…ç”Ÿæˆäº† {len(results)} ä¸ªé¡¹ç›®:")
        text_count = 0
        image_count = 0
        
        for i, item in enumerate(results):
            if isinstance(item, str):
                text_count += 1
                logger.info(f"  [{i}] ç”Ÿæˆæ–‡æœ¬: {item[:100]}{'...' if len(item) > 100 else ''}")
            elif hasattr(item, 'save'):  # PIL Image
                image_count += 1
                logger.info(f"  [{i}] ç”Ÿæˆå›¾åƒ: {item.size}")
                # ä¿å­˜å›¾åƒ
                image_path = f"overfitting_test_sample_{sample_idx + 1}/generated_image_{i}.png"
                os.makedirs(os.path.dirname(image_path), exist_ok=True)
                item.save(image_path)
                logger.info(f"       ä¿å­˜åˆ°: {image_path}")
        
        # ç®€å•çš„è¿‡æ‹Ÿåˆæ£€éªŒ
        expected_images = sum(1 for t in target_sequence if t == "<image>")
        expected_texts = sum(1 for t in target_sequence if isinstance(t, str) and t != "<image>")
        
        logger.info(f"\nè¿‡æ‹Ÿåˆæ£€éªŒ:")
        logger.info(f"  æœŸæœ›æ–‡æœ¬æ•°: {expected_texts}, å®é™…ç”Ÿæˆ: {text_count}")
        logger.info(f"  æœŸæœ›å›¾åƒæ•°: {expected_images}, å®é™…ç”Ÿæˆ: {image_count}")
        
        if image_count > 0 and expected_images > 0:
            logger.info("âœ… æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå›¾åƒï¼ˆç¬¦åˆè®­ç»ƒç›®æ ‡ï¼‰")
        elif expected_images > 0 and image_count == 0:
            logger.warning("âš ï¸  æœŸæœ›ç”Ÿæˆå›¾åƒä½†æœªç”Ÿæˆ")
        
        if text_count > 0 and expected_texts > 0:
            logger.info("âœ… æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬ï¼ˆç¬¦åˆè®­ç»ƒç›®æ ‡ï¼‰")
        
        return True
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ ·æœ¬ {sample_idx + 1} å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_special_tokens(engine):
    """æµ‹è¯•ç‰¹æ®Štokençš„è¯†åˆ«"""
    logger.info(f"\n{'='*50}")
    logger.info("æµ‹è¯•ç‰¹æ®Štokenè¯†åˆ«")
    logger.info(f"{'='*50}")
    
    # è·å–tokenizerå’Œç‰¹æ®Štoken
    tokenizer = engine.inferencer.tokenizer
    new_token_ids = engine.inferencer.new_token_ids
    
    logger.info("ç‰¹æ®Štokenæ˜ å°„:")
    for token_name, token_id in new_token_ids.items():
        if 'image' in token_name.lower() or 'vision' in token_name.lower():
            token_text = tokenizer.decode([token_id]) if token_id < len(tokenizer) else f"ID:{token_id}"
            logger.info(f"  {token_name}: {token_id} -> '{token_text}'")
    
    # æµ‹è¯•åŒ…å«ç‰¹æ®Štokençš„æ–‡æœ¬
    test_prompts = [
        "ç”»ä¸€åªçŒ« <|vision_start|> <|vision_end|>",
        "è¿™æ˜¯ä¸€æ®µæ–‡æœ¬ï¼Œç„¶åç”Ÿæˆå›¾åƒ",
        "åˆ›ä½œä¸€å¹…ç”»ä½œ"
    ]
    
    for i, prompt in enumerate(test_prompts):
        logger.info(f"\næµ‹è¯•æç¤º {i+1}: {prompt}")
        
        # ç¼–ç æµ‹è¯•
        try:
            encoded = tokenizer.encode(prompt)
            logger.info(f"  ç¼–ç é•¿åº¦: {len(encoded)}")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒç›¸å…³çš„ç‰¹æ®Štoken
            has_image_tokens = False
            for token_id in encoded:
                if token_id in new_token_ids.values():
                    token_name = [k for k, v in new_token_ids.items() if v == token_id][0]
                    if 'image' in token_name.lower() or 'vision' in token_name.lower():
                        has_image_tokens = True
                        logger.info(f"  åŒ…å«ç‰¹æ®Štoken: {token_name} (ID: {token_id})")
            
            if not has_image_tokens:
                logger.info("  æœªæ£€æµ‹åˆ°å›¾åƒç›¸å…³ç‰¹æ®Štoken")
                
        except Exception as e:
            logger.warning(f"  ç¼–ç å¤±è´¥: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª è¿‡æ‹Ÿåˆæƒé‡æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥ç‚¹è·¯å¾„
    checkpoint_path = "/workspace/bagel/results/unified_training_20250910_201937/checkpoints/0000800"
    
    # è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆæ‚¨éœ€è¦ä¿®æ”¹ä¸ºå®é™…è·¯å¾„ï¼‰
    training_data_paths = [
        "data/unified_train.jsonl",  # é»˜è®¤è·¯å¾„
        "training/sample_data.json",  # å¯èƒ½çš„è·¯å¾„
        "dataset/train.jsonl",  # å¦ä¸€ä¸ªå¯èƒ½è·¯å¾„
    ]
    
    # æ‰¾åˆ°å­˜åœ¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶
    training_data_path = None
    for path in training_data_paths:
        if os.path.exists(path):
            training_data_path = path
            break
    
    if training_data_path is None:
        logger.warning("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        training_data_path = "mock_data"
    
    logger.info(f"æ£€æŸ¥ç‚¹è·¯å¾„: {checkpoint_path}")
    logger.info(f"è®­ç»ƒæ•°æ®è·¯å¾„: {training_data_path}")
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        engine = create_inference_engine(checkpoint_path)
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç‰¹æ®Štoken
        test_special_tokens(engine)
        
        # åŠ è½½è®­ç»ƒæ•°æ®æ ·æœ¬
        samples = load_training_data_sample(training_data_path, num_samples=3)
        
        # æµ‹è¯•æ¯ä¸ªæ ·æœ¬
        success_count = 0
        for i, sample in enumerate(samples):
            success = test_single_sample(engine, sample, i)
            if success:
                success_count += 1
        
        # æ€»ç»“ç»“æœ
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ¯ è¿‡æ‹Ÿåˆæµ‹è¯•æ€»ç»“")
        logger.info(f"{'='*60}")
        logger.info(f"æµ‹è¯•æ ·æœ¬æ•°: {len(samples)}")
        logger.info(f"æˆåŠŸæµ‹è¯•: {success_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/len(samples)*100:.1f}%")
        
        if success_count == len(samples):
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•æ ·æœ¬éƒ½æˆåŠŸï¼æ¨¡å‹è¿‡æ‹Ÿåˆæ•ˆæœè‰¯å¥½ã€‚")
        elif success_count > 0:
            logger.info("âš ï¸  éƒ¨åˆ†æ ·æœ¬æµ‹è¯•æˆåŠŸï¼Œæ¨¡å‹éƒ¨åˆ†å­¦ä¼šäº†è®­ç»ƒæ•°æ®ã€‚")
        else:
            logger.warning("âŒ æ‰€æœ‰æ ·æœ¬æµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æ¨¡å‹æˆ–æ•°æ®ã€‚")
        
        # ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š
        generate_test_report(checkpoint_path, training_data_path, samples, success_count)
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def generate_test_report(checkpoint_path, data_path, samples, success_count):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    report = {
        "checkpoint_path": checkpoint_path,
        "training_data_path": data_path,
        "test_time": str(torch.datetime.datetime.now()),
        "total_samples": len(samples),
        "successful_samples": success_count,
        "success_rate": success_count/len(samples)*100 if samples else 0,
        "samples_tested": []
    }
    
    for i, sample in enumerate(samples):
        sample_info = {
            "sample_id": i + 1,
            "input_sequence": sample.get("input_sequence", []),
            "target_sequence": sample.get("target_sequence", []),
            "metadata": sample.get("metadata", {})
        }
        report["samples_tested"].append(sample_info)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "overfitting_test_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
